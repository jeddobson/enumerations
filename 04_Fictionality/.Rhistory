data(Angell)
Angell
View(Angell)
plot(Angell)
View(Angell)
#condition on group factor (region)
xyplot(moral~hetero|region, data=Angell)
#condition on group factor (region)
plot(moral~hetero|region, data=Angell)
#condition on group factor (region)
library(ggplot2)
ggplot(data=Angell, aes(hetero, moral))+
geom_point(size=4)+
facet_wrap(~region)+
geom_smooth(method="lm", se=FALSE)
data(Swiss)
data(swiss)
swiss
View(swiss)
plot(swiss)
View(swiss)
hist(swiss$Fertility)
hist(swiss$Education)
hist(swiss$Catholic)
plot(swiss$Catholic, swiss$Education)
plot(swiss$Catholic, log(swiss$Education))
plot(log(swiss$Catholic), log(swiss$Education))
model.0<-lm(log(swiss$Education)~log(swiss$Catholic))
summary(model.0)
plot(log(swiss$Education)~log(swiss$Catholic))
abline(model.0)
View(swiss)
plot(swiss$Agriculture, swiss$Education)
lm(swiss$Agriculture~swiss$Education)
model.0<-lm(swiss$Agriculture~swiss$Education)
summary(model.0)
abline(model.0)
model.0<-lm(swiss$Agriculture~swiss$Education)
plot(swiss$Agriculture~swiss$Education)
abline(model.0)
model.1<-lm(swiss$Agriculture~swiss$Education+swiss$Education^2)
abline(model.1, col="red")
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+(swiss$Education^2))
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+I(swiss$Education^2))
abline(model.1, col="red")
load("~/Documents/1. Articles/Socializations - Gender and Genre/gg.RData")
View(metadata.df)
View(pwc.all)
View(pwc.all)
View(pwc.all)
model.0<-lm(pwc ~ genre, data=pwc.all)
genre
View(pwc.all)
pwc.all$genre
pwc.all$pwc
as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc.all$pwc)
View(pwc.all)
pwc.all$pwc2<-as.numeric(as.character(pwc.all$pwc))
View(prev.all)
View(pwc.all)
model.0<-lm(as.numeric(pwc2) ~ genre, data=pwc.all)
model.0
summary(model.0)
pwc.all$genre
summary(model.0)
levela(pwc.all$genre)
levels(pwc.all$genre)
model.0<-lm(pwc2 ~ genre, data=pwc.all)
plot(pwc2 ~ genre, data=pwc.all)
View(pwc.all)
model.1<-lm(as.numeric(pwc2) ~ genre*ag, data=pwc.all)
plot(model.1)
model.1<-lm(as.numeric(pwc2) ~ genre+ag, data=pwc.all)
summary(model.1)
plot(pwc2 ~ genre+ag, data=pwc.all)
plot(pwc2 ~ genre:ag, data=pwc.all)
model.1<-lm(pwc2 ~ genre:ag, data=pwc.all)
summary(model.1)
levels(pwc.all$ag)
table(pwc.all$ag)
library(car)
library(effects)
install.packages("effects")
install.packages("interplot")
data("Leinhardt")
l$oilno<-2
l<-Leinhardt
View(Leinhardt)
hist(l$income)
hist(l$infant)
View(Leinhardt)
plot(l$infant~l$income)
plot(l$infant~log(l$income))
plot(log(l$infant)~log(l$income))
summary(lm(log(l$infant)~log(l$income)))
abline(lm(log(l$infant)~log(l$income)))
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality")
filenames<-list.files("LIWC_Novel_Segments")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality/LIWC_Novel_Segments")
final.df<-NULL
for (i in 1:length(filenames)){
print(i)
#ingest the LIWC table
a<-read.csv(filenames[i], sep = "\t")
#Label your data by corpus
label.df<-data.frame(a$Filename)
colnames(label.df)<-c("filenames")
label.df<-cSplit(label.df, 'filenames', sep="_", type.convert=FALSE)
label.df<-cSplit(label.df, 'filenames_5', sep=".", type.convert=FALSE)
a$corpus<-label.df$filenames_5_1
a<-a[a$corpus == "CT" | a$corpus == "HIST", ]
df<-a
#create folds for cross-validation
folds<-createFolds(df$corpus, k=10) # k = number of folds (1-10) where 10 is best
#run 10-fold cross validation
cv.results<-lapply(folds, function(x){
df.train<-df[-x,]
df.test<-df[x,]
df.model<-ksvm(corpus ~ ., data=df.train, kernel="rbfdot")
df.pred<-predict(df.model, df.test)
con.matrix<-confusionMatrix(df.pred, df.test$corpus, positive = "CT")
f1<-con.matrix$byClass[[7]]
#p<-con.matrix$overall[[6]]
})
#calculate the avg F1 score for all folds
F1.score<-mean(unlist(cv.results))
seg<-as.numeric(label.df$filenames_1)[1]
temp.df<-data.frame(seg, F1.score)
final.df<-rbind(final.df, temp.df)
}
library(splitstackshape)
final.df<-NULL
for (i in 1:length(filenames)){
print(i)
#ingest the LIWC table
a<-read.csv(filenames[i], sep = "\t")
#Label your data by corpus
label.df<-data.frame(a$Filename)
colnames(label.df)<-c("filenames")
label.df<-cSplit(label.df, 'filenames', sep="_", type.convert=FALSE)
label.df<-cSplit(label.df, 'filenames_5', sep=".", type.convert=FALSE)
a$corpus<-label.df$filenames_5_1
a<-a[a$corpus == "CT" | a$corpus == "HIST", ]
df<-a
#create folds for cross-validation
folds<-createFolds(df$corpus, k=10) # k = number of folds (1-10) where 10 is best
#run 10-fold cross validation
cv.results<-lapply(folds, function(x){
df.train<-df[-x,]
df.test<-df[x,]
df.model<-ksvm(corpus ~ ., data=df.train, kernel="rbfdot")
df.pred<-predict(df.model, df.test)
con.matrix<-confusionMatrix(df.pred, df.test$corpus, positive = "CT")
f1<-con.matrix$byClass[[7]]
#p<-con.matrix$overall[[6]]
})
#calculate the avg F1 score for all folds
F1.score<-mean(unlist(cv.results))
seg<-as.numeric(label.df$filenames_1)[1]
temp.df<-data.frame(seg, F1.score)
final.df<-rbind(final.df, temp.df)
}
library("kernlab")
library("caret")
final.df<-NULL
for (i in 1:length(filenames)){
print(i)
#ingest the LIWC table
a<-read.csv(filenames[i], sep = "\t")
#Label your data by corpus
label.df<-data.frame(a$Filename)
colnames(label.df)<-c("filenames")
label.df<-cSplit(label.df, 'filenames', sep="_", type.convert=FALSE)
label.df<-cSplit(label.df, 'filenames_5', sep=".", type.convert=FALSE)
a$corpus<-label.df$filenames_5_1
a<-a[a$corpus == "CT" | a$corpus == "HIST", ]
df<-a
#create folds for cross-validation
folds<-createFolds(df$corpus, k=10) # k = number of folds (1-10) where 10 is best
#run 10-fold cross validation
cv.results<-lapply(folds, function(x){
df.train<-df[-x,]
df.test<-df[x,]
df.model<-ksvm(corpus ~ ., data=df.train, kernel="rbfdot")
df.pred<-predict(df.model, df.test)
con.matrix<-confusionMatrix(df.pred, df.test$corpus, positive = "CT")
f1<-con.matrix$byClass[[7]]
#p<-con.matrix$overall[[6]]
})
#calculate the avg F1 score for all folds
F1.score<-mean(unlist(cv.results))
seg<-as.numeric(label.df$filenames_1)[1]
temp.df<-data.frame(seg, F1.score)
final.df<-rbind(final.df, temp.df)
}
warnings()
final.sort<-final.df[order(final.df$seg),]
View(final.sort)
con.matrix$byClass
folds<-createFolds(df$corpus, k=10) # k = number of folds (1-10) where 10 is best
folds
x<-01
df.train<-df[-x,]
df.test<-df[x,]
df.model<-ksvm(corpus ~ ., data=df.train, kernel="rbfdot")
df.pred<-predict(df.model, df.test)
con.matrix<-confusionMatrix(df.pred, df.test$corpus, positive = "CT")
i
a<-read.csv(filenames[i], sep = "\t")
#Label your data by corpus
label.df<-data.frame(a$Filename)
colnames(label.df)<-c("filenames")
label.df<-cSplit(label.df, 'filenames', sep="_", type.convert=FALSE)
label.df<-cSplit(label.df, 'filenames_5', sep=".", type.convert=FALSE)
a$corpus<-label.df$filenames_5_1
a<-a[a$corpus == "CT" | a$corpus == "HIST", ]
df<-a
folds<-createFolds(df$corpus, k=10) # k = number of folds (1-10) where 10 is best
df.train<-df[-x,]
df.test<-df[x,]
df.model<-ksvm(corpus ~ ., data=df.train, kernel="rbfdot")
df.pred<-predict(df.model, df.test)
con.matrix<-confusionMatrix(df.pred, df.test$corpus, positive = "CT")
folds
View(df)
i
filenames
seg<-as.numeric(label.df$filenames_1)[1]
seg
df.train
View(df.train)
df$corpus
levels(factor(df$corpus))
df.test$corpus
df.test<-df[x,]
View(df.test)
folds
x<-folds$Fold01
df.train<-df[-x,]
df.test<-df[x,]
df.model<-ksvm(corpus ~ ., data=df.train, kernel="rbfdot")
df.pred<-predict(df.model, df.test)
con.matrix<-confusionMatrix(df.pred, df.test$corpus, positive = "CT")
con.matrix
con.matrix$byClass
con.matrix$byClass[[7]]
ggplot(final.sort, aes(x=seg, y=F1.score)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
geom_point(shape=22, size=2, fill="grey") +
geom_line() +
ylim(0.75,1.0) +
labs(x="Number of Words", y="Accuracy (F1)", caption="\nFig. 4.1 Accuracy of predicting fictional texts using an increasing\n number of words from the beginning of a document.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
#
final.sort<-read.csv("Novel_Segment.csv")
ggplot(final.sort, aes(x=seg, y=F1.score)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
geom_point(shape=22, size=2, fill="grey") +
geom_line() +
ylim(0.75,1.0) +
labs(x="Number of Words", y="Accuracy (F1)", caption="\nFig. 4.1 Accuracy of predicting fictional texts using an increasing\n number of words from the beginning of a document.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
#
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality")
final.sort<-read.csv("Novel_Segment.csv")
ggplot(final.sort, aes(x=seg, y=F1.score)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
geom_point(shape=22, size=2, fill="grey") +
geom_line() +
ylim(0.75,1.0) +
labs(x="Number of Words", y="Accuracy (F1)", caption="\nFig. 4.1 Accuracy of predicting fictional texts using an increasing\n number of words from the beginning of a document.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
#labs(x="Words", y="Accuracy")
library(wordcloud)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality/Wordcloud_Tables")
a<-read.csv("Piper_Supplement_Table4.1_LIWC_19C_Fiction.csv")
a$Feature<-tolower(a$Feature)
#make wordcloud
nlevels(a$Category)
basecolors = c("black", "gray50", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$ratio, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
a<-read.csv("Piper_Appendix_Table4.2.csv")
a$Feature<-tolower(a$Feature)
#remove exclamation marks!
a<-a[-2,]
#make wordcloud
nlevels(factor(a$Category))
basecolors = c("black", "gray50", "black", "gray50", "gray50", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$Ratio, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
a<-read.csv("Piper_Supplement_Table4.2_LIWC_19C_Fiction_DialogueRemoved.csv")
a$Feature<-tolower(a$Feature)
#remove exclamation marks!
a<-a[-2,]
#make wordcloud
nlevels(factor(a$Category))
basecolors = c("black", "gray50", "black", "gray50", "gray50", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$Ratio, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
a<-read.csv("Piper_Appendix_Table4.2.csv")
a$Feature<-tolower(a$Feature)
#remove exclamation marks!
a<-a[-2,]
#make wordcloud
nlevels(factor(a$Category))
basecolors = c("black", "gray50", "black", "gray50", "gray50", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$Ratio, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
a<-read.csv("Piper_Supplement_Table4.3_LIWC_19C_Novels_3P_v_Histories.csv")
a$Feature<-tolower(a$Feature)
#remove exclamation marks!
a<-a[-1,]
#make wordcloud
nlevels(factor(a$Category))
basecolors.a = c("black", "gray50", "black", "gray50", "gray50", "gray50", "gray50")
group.a = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist.a = basecolors.a[ match(group.a,unique(group.a)) ]
b<-read.csv("Piper_Supplement_Table4.4_LIWC_19C_German_NoDialogueRemoved.csv")
b$Feature<-tolower(b$Feature)
#remove exclamation marks
b<-b[-2,]
#make wordcloud
nlevels(factor(b$Category))
basecolors.b = c("gray50", "black", "gray50", "black", "gray50", "gray50")
group.b = as.character(b$Category)
# find position of group in list of groups, and select that matching color...
colorlist.b = basecolors.b[ match(group.b,unique(group.b)) ]
c<-read.csv("Piper_Supplement_Table4.5_LIWC_Contemporary_Fiction_DialogueRemoved.csv")
c$Feature<-tolower(c$Feature)
c<-c[1:20,]
c$Category<-factor(c$Category)
#make wordcloud
nlevels(c$Category)
basecolors.c = c("gray50", "black", "black", "gray50", "gray50")
group.c = as.character(c$Category)
# find position of group in list of groups, and select that matching color...
colorlist.c = basecolors.c[ match(group.c,unique(group.c)) ]
par(mfrow=c(3,1))
wordcloud(a$Feature, a$Ratio, scale=c(3,0.05), min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist.a)
wordcloud(b$Feature, b$Ratio, scale=c(4,1.5),min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist.b)
wordcloud(c$Feature, c$Ratio, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist.c)
par(mfrow=c(3,1))
wordcloud(a$Feature, a$Ratio, scale=c(3,0.05), min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist.a)
wordcloud(b$Feature, b$Ratio, scale=c(4,1.5),min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist.b)
wordcloud(c$Feature, c$Ratio, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist.c)
a<-read.csv("Piper_Supplement_Table4.6_LIWC_Novels_v_Fiction.csv")
a$Feature<-tolower(a$Feature)
#make wordcloud
nlevels(a$Category)
basecolors = c("grey50", "grey50", "black", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$Ratio^2, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
dev.off()
a<-read.csv("Piper_Supplement_Table4.6_LIWC_Novels_v_Fiction.csv")
a$Feature<-tolower(a$Feature)
#make wordcloud
nlevels(a$Category)
basecolors = c("grey50", "grey50", "black", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$Ratio^2, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
a<-read.csv("Piper_Supplement_Table4.6_LIWC_Novels_v_Fiction.csv")
a$Feature<-tolower(a$Feature)
#make wordcloud
nlevels(a$Category)
basecolors = c("grey50", "grey50", "black", "gray50", "gray50")
group = as.character(a$Category)
# find position of group in list of groups, and select that matching color...
colorlist = basecolors[ match(group,unique(group)) ]
wordcloud(a$Feature, a$Ratio^2, min.freq = 0, rot.per = 0, random.order = F, ordered.colors=TRUE, colors=colorlist)
library("tm")
library("SnowballC")
dict<-read.csv("dict_sense_stem_all.csv", header=F, stringsAsFactors = F)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/Data/Chap4")
dict<-read.csv("dict_sense_stem_all.csv", header=F, stringsAsFactors = F)
gc()
dict<-read.csv("dict_sense_stem_all.csv", header=F, stringsAsFactors = F)
corpus1 <- VCorpus(DirSource("19C_EN_NOV_3P_NoQuotes"), readerControl=list(language="English"))
#clean data
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, stemDocument, language = "english")
#create document term matrix
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
#perform transformations on the raw counts
scaling1<-rowSums(corpus1.matrix) #get total word counts for each work
corpus2 <- VCorpus(DirSource("19C_EN_HIST_NoQuotes"), readerControl=list(language="English"))
corpus2 <- tm_map(corpus2, content_transformer(stripWhitespace))
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, content_transformer(removeNumbers))
corpus2 <- tm_map(corpus2, content_transformer(removePunctuation))
corpus2 <- tm_map(corpus2, stemDocument, language = "english")
#create DTM
corpus2.dtm<-DocumentTermMatrix(corpus2, control=list(wordLengths=c(1,Inf)))
corpus2.matrix<-as.matrix(corpus2.dtm, stringsAsFactors=F)
#scale
scaling2<-rowSums(corpus2.matrix) #get total word counts for each work
corpus1.sub<-corpus1.matrix[,colnames(corpus1.matrix) %in% dict$V1]
corpus2.sub<-corpus2.matrix[,colnames(corpus2.matrix) %in% dict$V1]
corpus1.sum<-data.frame(rowSums(corpus1.sub))
corpus2.sum<-data.frame(rowSums(corpus2.sub))
corpus1.scaled<-corpus1.sum/scaling1
corpus2.scaled<-corpus2.sum/scaling2
hist(corpus1.scaled[,1])
shapiro.test(corpus1.scaled[,1])
hist(corpus2.scaled[,1])
shapiro.test(corpus2.scaled[,1])
View(corpus1.scaled)
wilcox.test(corpus1.scaled[,1], corpus2.scaled[,1])
median(corpus1.scaled[,1])/median(corpus2.scaled[,1])
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality")
klab<-read.csv("Emotion_Sense_20C.csv")
stan<-read.csv("Emotion_Sense_19C")
klab<-read.csv("Emotion_Sense_20C.csv")
gc()
klab<-read.csv("Emotion_Sense_20C.csv")
stan<-read.csv("Emotion_Sense_19C")
stan<-read.csv("Emotion_Sense_19C.csv")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality/LIWC_Feature_Tables")
stan.meta<-read.csv("LIWC_19C_STANFORD.csv")
klab.meta<-read.csv("LIWC_20C_KLAB_RANDOM_META.csv")
stan.sub<-stan[which(as.character(stan$X) %in% as.character(stan.meta$Filename)), ]
stan.sub<-stan.sub[order(as.character(stan.sub$X)),]
stan.meta<-stan.meta[order(as.character(stan.meta$Filename)),]
which(as.character(stan.sub$X) != as.character(stan.meta$Filename))
stan.sub<-cbind(stan.sub, stan.meta$date)
klab.sub<-klab[which(as.character(klab$X) %in% as.character(klab.meta$Filename)), ]
klab.sub<-klab.sub[order(as.character(klab.sub$X)),]
klab.meta<-klab.meta[order(as.character(klab.meta$Filename)),]
which(as.character(klab.sub$X) != as.character(klab.meta$Filename))
klab.sub<-cbind(klab.sub, klab.meta$date)
colnames(stan.sub)<-c("filename", "emotion", "type1", "sense", "type2", "date")
colnames(klab.sub)<-c("filename", "emotion", "type1", "sense", "type2", "date")
all.df<-rbind(stan.sub, klab.sub)
all.df<-all.df[which(all.df$date > 1799),]
klab.meta<-read.csv("LIWC_20C_KLAB_RANDOM.csv")
klab.sub<-klab[which(as.character(klab$X) %in% as.character(klab.meta$Filename)), ]
klab.sub<-klab.sub[order(as.character(klab.sub$X)),]
klab.meta<-klab.meta[order(as.character(klab.meta$Filename)),]
which(as.character(klab.sub$X) != as.character(klab.meta$Filename))
klab.sub<-cbind(klab.sub, klab.meta$date)
View(klab.sub)
View(klab)
View(klab.meta)
stan.sub<-stan[which(as.character(stan$X) %in% as.character(stan.meta$Filename)), ]
stan.sub<-stan.sub[order(as.character(stan.sub$X)),]
stan.meta<-stan.meta[order(as.character(stan.meta$Filename)),]
which(as.character(stan.sub$X) != as.character(stan.meta$Filename))
stan.sub<-cbind(stan.sub, stan.meta$date)
klab.sub<-klab[which(as.character(klab$X) %in% as.character(klab.meta$Filename)), ]
klab.sub<-klab.sub[order(as.character(klab.sub$X)),]
klab.meta<-klab.meta[order(as.character(klab.meta$Filename)),]
which(as.character(klab.sub$X) != as.character(klab.meta$Filename))
klab.sub<-cbind(klab.sub, klab.meta$date)
colnames(stan.sub)<-c("filename", "emotion", "type1", "sense", "type2", "date")
colnames(klab.sub)<-c("filename", "emotion", "type1", "sense", "type2", "date")
all.df<-rbind(stan.sub, klab.sub)
all.df<-all.df[which(all.df$date > 1799),]
final.df<-NULL
for (i in 1:nlevels(factor(all.df$date))){
sub<-all.df[all.df$date == levels(factor(all.df$date))[i],]
perception<-mean(sub$sense)*10000
emotion<-mean(sub$emotion)*10000
score<-c(perception, emotion)
type<-c("perception", "emotion")
year<-sub$date[1]
temp.df<-data.frame(year, score, type)
final.df<-rbind(final.df, temp.df)
}
ggplot(final.df, aes(x=year, y=score, shape=type)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(legend.position="bottom") +
theme(legend.title=element_blank()) +
theme(legend.key = element_rect(fill = NA)) +
theme(legend.text=element_text(size=12)) +
geom_point() +
scale_shape_manual(values=c(17,1)) +
#geom_line() +
labs(x="Words", y="Words (Per 10K)", caption="\nFig. 4.7 Frequency of words related to emotions and sense perception\nin English novels, 1800-2000\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)") +
#labs(x="Year", y="Words (Per 10K)") +
stat_smooth(method=loess, colour="black")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/04. Fictionality")
klab.txt<-read.csv("Emotion_Sense_20C.csv")
stan.txt<-read.csv("Emotion_Sense_19C.csv")
klab.nrc<-read.csv("Emotion_NRC_20C.csv")
stan.nrc<-read.csv("Emotion_NRC_19C.csv")
cor.test(stan.nrc$emotion, stan.txt$emotion)
cor.test(klab.nrc$emotion, klab.txt$emotion)
plot(stan.nrc$emotion, stan.txt$emotion, xlim=c(0.05, 0.15))
plot(klab.nrc$emotion, klab.txt$emotion)
