library(openNLP)
library(rJava)
load("~/Documents/1. Articles/Socializations - Gender and Genre/perm_data.RData")
library(splitstackshape)
library(epiR)
library(ggplot2)
library(stats)
library(DTK)
library(igraph)
library(RColorBrewer)
library(dplyr)
library(reshape2)
library(stringr)
perm.ALL
perm.ALL<-data.frame(rbind(perm.YA, perm.NYT, perm.MY, perm.SF, perm.BS, perm.ROM, perm.PW))
View(perm.ALL)
quantile(perm.ALL$mean.int.all, 0.05)
mean(assortativity.ALL$assort.int.all, na.rm=T)
min(perm.ALL$mean.int.all)
mean(assortativity.ALL$assort.int.all, na.rm=T) < min(perm.ALL$mean.int.all)
mean(assortativity.ALL$assort.int.20, na.rm=T) < min(perm.ALL$mean.int.20)
mean(assortativity.ALL$assort.int.10, na.rm=T) < min(perm.ALL$mean.int.10)
mean(assortativity.ALL$assort.rel.all, na.rm=T) < quantile(perm.ALL$mean.rel.all, 0.05)
mean(assortativity.ALL$assort.rel.all, na.rm=T)
summary(perm.ALL$mean.rel.all)
View(assortativity.ALL)
View(assortativity.ALL)
View(perm.ALL)
hist(perm.ALL$mean.int.all)
hist(perm.ALL$mean.int.20)
hist(perm.ALL$mean.int.10)
library(car)
install.packages("car")
data(Robey)
library(car)
data(Robey)
model.0<-lm(tfr~contraceptors, data=Robey)
View(Robey)
summary(model.0)
plot(model.0)
plot(tfr~contraceptors, data=Robey)
confint(model.0)
abline(model.0)
View(model.0)
View(Robey)
model.0$coefficients[[1]]
model.0$coefficients[[2]]
#how can you get predicted values for a given x?
pred.x<-Robey$contraceptors*model.0$coefficients[[2]]
pred.x
plot(pred.x)
Robey$contraceptors
View(Robey)
plot(sort(pred.x))
pred.x<-model.0$coefficients[[1]]+Robey$contraceptors*model.0$coefficients[[2]]
plot(sort(pred.x))
plot(sort(pred.x, decreasing = T))
#how can you get predicted values for a given x?
pred.x<-model.0$coefficients[[1]]+(Robey$contraceptors*model.0$coefficients[[2]])
plot(sort(pred.x, decreasing = T))
coefficients[[1]]
model.0$coefficients[[1]]
model.0$coefficients[[2]]
Robey$contraceptors
plot(pred.x)
View(Robey)
plot(Robey$contraceptors,pred.x)
pred.x
plot(model.0)
plot(Robey$contraceptors,pred.x)
pred.y<-model.0$coefficients[[1]]+(Robey$contraceptors*model.0$coefficients[[2]])
plot(Robey$contraceptors,pred.y)
rm(pred.x)
plot(model.0)
fitted(model.0)
fit.y<-fitted(model.0)
plot(fit.y)
plot(Robey$contraceptors,fit.y)
pred.y
View(Robey)
abs(pred.y-Robey$tfr)
(pred.y-Robey$tfr)^2
res<-(pred.y-Robey$tfr)^2
plot(Robey$contraceptors, res)
res
which(res == max(res))
res[12]
View(Robey)
Robey[12,]
plot(tfr~contraceptors, data=Robey)
abline(model.0)
#plot line segments between observed and predicted values
segments(Robey$contraceptors, Robey$tfr, Robey$contraceptors, pred.y)
residuals(model.0)
#calculate residuals
res<-(pred.y-Robey$tfr)
plot(Robey$contraceptors, res)
#trial 2
data("anscombe")
#trial 2
data(anscombe)
anscombe
model.0<-lm(anscombe$y1~anscombe$x1)
coef(model.0)
summary(model.0)
#trial3
data("Leinhardt")
Leinhardt
View(Leinhardt)
hist(Leinhardt$income)
hist(Leinhardt$infant)
rug(Leinhardt$income)
hist(Leinhardt$income)
rug(Leinhardt$income)
rug(Leinhardt$income, color="red")
rug(Leinhardt$income, col="red")
hist(Leinhardt$income)
hist(Leinhardt$income)
rug(Leinhardt$income, col="red")
hist(Leinhardt$infant)
rug(Leinhardt$infant, col="blue")
#trial4
#multivariate data
data(Angell)
Angell
View(Angell)
plot(Angell)
View(Angell)
#condition on group factor (region)
xyplot(moral~hetero|region, data=Angell)
#condition on group factor (region)
plot(moral~hetero|region, data=Angell)
#condition on group factor (region)
library(ggplot2)
ggplot(data=Angell, aes(hetero, moral))+
geom_point(size=4)+
facet_wrap(~region)+
geom_smooth(method="lm", se=FALSE)
data(Swiss)
data(swiss)
swiss
View(swiss)
plot(swiss)
View(swiss)
hist(swiss$Fertility)
hist(swiss$Education)
hist(swiss$Catholic)
plot(swiss$Catholic, swiss$Education)
plot(swiss$Catholic, log(swiss$Education))
plot(log(swiss$Catholic), log(swiss$Education))
model.0<-lm(log(swiss$Education)~log(swiss$Catholic))
summary(model.0)
plot(log(swiss$Education)~log(swiss$Catholic))
abline(model.0)
View(swiss)
plot(swiss$Agriculture, swiss$Education)
lm(swiss$Agriculture~swiss$Education)
model.0<-lm(swiss$Agriculture~swiss$Education)
summary(model.0)
abline(model.0)
model.0<-lm(swiss$Agriculture~swiss$Education)
plot(swiss$Agriculture~swiss$Education)
abline(model.0)
model.1<-lm(swiss$Agriculture~swiss$Education+swiss$Education^2)
abline(model.1, col="red")
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+(swiss$Education^2))
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+I(swiss$Education^2))
abline(model.1, col="red")
load("~/Documents/1. Articles/Socializations - Gender and Genre/gg.RData")
View(metadata.df)
View(pwc.all)
View(pwc.all)
View(pwc.all)
model.0<-lm(pwc ~ genre, data=pwc.all)
genre
View(pwc.all)
pwc.all$genre
pwc.all$pwc
as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc.all$pwc)
View(pwc.all)
pwc.all$pwc2<-as.numeric(as.character(pwc.all$pwc))
View(prev.all)
View(pwc.all)
model.0<-lm(as.numeric(pwc2) ~ genre, data=pwc.all)
model.0
summary(model.0)
pwc.all$genre
summary(model.0)
levela(pwc.all$genre)
levels(pwc.all$genre)
model.0<-lm(pwc2 ~ genre, data=pwc.all)
plot(pwc2 ~ genre, data=pwc.all)
View(pwc.all)
model.1<-lm(as.numeric(pwc2) ~ genre*ag, data=pwc.all)
plot(model.1)
model.1<-lm(as.numeric(pwc2) ~ genre+ag, data=pwc.all)
summary(model.1)
plot(pwc2 ~ genre+ag, data=pwc.all)
plot(pwc2 ~ genre:ag, data=pwc.all)
model.1<-lm(pwc2 ~ genre:ag, data=pwc.all)
summary(model.1)
levels(pwc.all$ag)
table(pwc.all$ag)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/03. Topoi")
load("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/03. Topoi/Novel_German_Topic150_Enumerations.RData")
final.df<-NULL
for (i in 1:ncol(probabilities2)){
print(i)
topic.no<-i
#no. tokens
#takes the top 20 words and counts their overall frequency in the corpus
tok.sub<-corpus.sparse.dtm[,which(colnames(corpus.sparse.dtm) %in% as.character(words.topic[,topic.no]))]
no.tokens<-sum(tok.sub)
#no. documents
#counts the number of documents that exhibit a given topic above some artificial threshold (here 0.2)
doc.sub<-data.frame(row.names(probabilities2), probabilities2[,topic.no])
doc.sub<-doc.sub[which(doc.sub$probabilities2...topic.no. > 0.2),]
no.docs<-nrow(doc.sub)
if (no.docs > 5){
#no. novels
#counts how many novels the documents belong to
nov.sub<-cSplit(doc.sub, "row.names.probabilities2.", sep="_")
no.novels<-nlevels(factor(nov.sub$row.names.probabilities2._3))
#heterogeneity
#finds the number of documents that belong to the single most dominant novel to see the extent to
#which the topic is dominated by a single novel
heterogeneity<-1-max(table(factor(nov.sub$row.names.probabilities2._3)))/nrow(nov.sub)
#avg. date
#average date of the documents
avg.date<-round(mean(nov.sub$row.names.probabilities2._1))
#sd.date
#standard deviation of the document dates
sd.date<-round(sd(nov.sub$row.names.probabilities2._1))
#coherence
#see Mimno's article -- measures co-document frequency relative to document frequency
#the more words appear together in documents (co-document frequency) versus in a single document (doc frequency)
#the more semantically coherent the topic
tdm<-as.matrix(t(corpus.sparse.dtm))
tdm<-tdm[,colnames(tdm) %in% as.character(doc.sub$row.names.probabilities2.)] #keep docs in the top topic list
tdm<-tdm[row.names(tdm) %in% as.character(words.topic[,topic.no]),] #keep only top 20 topic words
russel.dist<-as.matrix(simil(tdm, method = "Russel", convert_distances = TRUE))
russel.final<-russel.dist*ncol(tdm)
russel.final[is.na(russel.final)]<-0
coherence.total<-0
for (k in 1:nrow(tdm)) {
doc.freq<-length(which(tdm[k,] != 0))
vec1<-0
for (m in 1:nrow(russel.final)) {
if (russel.final[k,m] != 0){
co.doc.freq<-as.integer(russel.final[k,m])
coherence1<-log((co.doc.freq+1)/doc.freq)
vec1<-vec1 + coherence1
}
}
coherence.total<-coherence.total + vec1
}
coherence.total<-abs(coherence.total)
#avg.sim
#calculate the average pairwise similarity between documents in this topic using all their words
#similarity is measured using correlation
dtm<-corpus.sparse.dtm[row.names(corpus.sparse.dtm) %in% as.character(doc.sub$row.names.probabilities2.),]
dtm<-as.matrix(dtm[,col_sums(dtm) != 0])
dtm.sim<-as.matrix(simil(dtm, method = "correlation"))
sim.score<-mean(dtm.sim[lower.tri(dtm.sim)])
#store in data frame
temp.df<-data.frame(topic.no, no.tokens, no.docs, no.novels, heterogeneity, avg.date, sd.date, coherence.total, sim.score)
final.df<-rbind(final.df, temp.df)
}
}
library("tm")
library("proxy")
library("SnowballC")
library("stats")
library("topicmodels")
library(slam)
library(splitstackshape)
final.df<-NULL
for (i in 1:ncol(probabilities2)){
print(i)
topic.no<-i
#no. tokens
#takes the top 20 words and counts their overall frequency in the corpus
tok.sub<-corpus.sparse.dtm[,which(colnames(corpus.sparse.dtm) %in% as.character(words.topic[,topic.no]))]
no.tokens<-sum(tok.sub)
#no. documents
#counts the number of documents that exhibit a given topic above some artificial threshold (here 0.2)
doc.sub<-data.frame(row.names(probabilities2), probabilities2[,topic.no])
doc.sub<-doc.sub[which(doc.sub$probabilities2...topic.no. > 0.2),]
no.docs<-nrow(doc.sub)
if (no.docs > 5){
#no. novels
#counts how many novels the documents belong to
nov.sub<-cSplit(doc.sub, "row.names.probabilities2.", sep="_")
no.novels<-nlevels(factor(nov.sub$row.names.probabilities2._3))
#heterogeneity
#finds the number of documents that belong to the single most dominant novel to see the extent to
#which the topic is dominated by a single novel
heterogeneity<-1-max(table(factor(nov.sub$row.names.probabilities2._3)))/nrow(nov.sub)
#avg. date
#average date of the documents
avg.date<-round(mean(nov.sub$row.names.probabilities2._1))
#sd.date
#standard deviation of the document dates
sd.date<-round(sd(nov.sub$row.names.probabilities2._1))
#coherence
#see Mimno's article -- measures co-document frequency relative to document frequency
#the more words appear together in documents (co-document frequency) versus in a single document (doc frequency)
#the more semantically coherent the topic
tdm<-as.matrix(t(corpus.sparse.dtm))
tdm<-tdm[,colnames(tdm) %in% as.character(doc.sub$row.names.probabilities2.)] #keep docs in the top topic list
tdm<-tdm[row.names(tdm) %in% as.character(words.topic[,topic.no]),] #keep only top 20 topic words
russel.dist<-as.matrix(simil(tdm, method = "Russel", convert_distances = TRUE))
russel.final<-russel.dist*ncol(tdm)
russel.final[is.na(russel.final)]<-0
coherence.total<-0
for (k in 1:nrow(tdm)) {
doc.freq<-length(which(tdm[k,] != 0))
vec1<-0
for (m in 1:nrow(russel.final)) {
if (russel.final[k,m] != 0){
co.doc.freq<-as.integer(russel.final[k,m])
coherence1<-log((co.doc.freq+1)/doc.freq)
vec1<-vec1 + coherence1
}
}
coherence.total<-coherence.total + vec1
}
coherence.total<-abs(coherence.total)
#avg.sim
#calculate the average pairwise similarity between documents in this topic using all their words
#similarity is measured using correlation
dtm<-corpus.sparse.dtm[row.names(corpus.sparse.dtm) %in% as.character(doc.sub$row.names.probabilities2.),]
dtm<-as.matrix(dtm[,col_sums(dtm) != 0])
dtm.sim<-as.matrix(simil(dtm, method = "correlation"))
sim.score<-mean(dtm.sim[lower.tri(dtm.sim)])
#store in data frame
temp.df<-data.frame(topic.no, no.tokens, no.docs, no.novels, heterogeneity, avg.date, sd.date, coherence.total, sim.score)
final.df<-rbind(final.df, temp.df)
}
}
View(final.df)
topic.no<-150
doc.sub<-data.frame(row.names(probabilities2), probabilities2[,topic.no])
doc.sub<-doc.sub[which(doc.sub$probabilities2...topic.no. > 0.2),]
topic.dtm<-corpus.sparse.dtm[row.names(corpus.sparse.dtm) %in% as.character(doc.sub$row.names.probabilities2.),] #keep docs in the top topic list
topic.dtm<-topic.dtm[,colnames(topic.dtm) %in% as.character(words.topic[,topic.no])] #keep only top 20 topic words
topic.matrix<-as.matrix(topic.dtm)
topic.dist<-simil(topic.matrix, method="cosine")
topic.dist<-pr_simil2dist(topic.dist)
fit<-hclust(topic.dist, method = "ward.D2") #other options for "ward.D2" = "complete", "single", or "centroid" (there are more...)
plot(fit)
#predict optimal number of clusters
library(dendextend)
fit_k<-find_k(fit, krange = 2:(nrow(topic.dist)-1))
#identify clusters in the dendogram
rect.hclust(fit, k=fit_k$nc, border="red")
### Fig. 3.1 ###
#dendogram of the documents in Topic 150
plot(as.dendogram(fit), horizontal=T)
### Fig. 3.1 ###
#dendrogram of the documents in Topic 150
plot(as.dendrogram(fit), horizontal=T)
### Fig. 3.1 ###
#dendrogram of the documents in Topic 150
plot(as.dendrogram(fit), horiz=T)
### Fig. 3.1 ###
#dendrogram of the documents in Topic 150
plot(as.dendrogram(fit))
fit_k<-find_k(fit, krange = 2:(nrow(topic.dist)-1))
rect.hclust(fit, k=fit_k$nc, border="red")
topic.no<-150
#subset by topic words
doc.sub<-data.frame(row.names(probabilities2), probabilities2[,topic.no])
#subset by documents
doc.sub<-doc.sub[which(doc.sub$probabilities2...topic.no. > 0.2),]
#create term document matrix
topic.dtm<-corpus.sparse.dtm[row.names(corpus.sparse.dtm) %in% as.character(doc.sub$row.names.probabilities2.),] #keep docs in the top topic list
topic.dtm<-topic.dtm[,colnames(topic.dtm) %in% as.character(words.topic[,topic.no])] #keep only top 20 topic words
topic.matrix<-as.matrix(topic.dtm)
tdm<-t(topic.matrix)
#create similarity matrix
tdm.dist<-simil(tdm, method="correlation")
#transform to distance matrix for clustering
tdm.dist<-pr_simil2dist(tdm.dist)
#establish clusters w hierarchical clustering (using k=2)
fit <- hclust(tdm.dist, method="ward.D2")
groups <- cutree(fit, k=2) # cut tree into k clusters
coords <- cmdscale(tdm.dist, eig = TRUE, k = 2)
x <- coords$points[, 1]
y <- coords$points[, 2]
#build data frame
mds<-data.frame(x, y, row.names(coords$points), unname(groups))
colnames(mds)<-c("x", "y", "words", "cluster")
library(ggplot2)
mds<-read.csv("Topic150_Coordinates_MDS.csv")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8))
mds<-read.csv("Topic150_Coordinates_MDS.csv")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8))
labs(caption="\nFig. 3.2 Correlation of Topic 150 words\n\nSource: Andrew Piper, Enumerations: The Quantities of Literature (2018)")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8)) +
labs(caption="\nFig. 3.2 Correlation of Topic 150 words\n\nSource: Andrew Piper, Enumerations: The Quantities of Literature (2018)")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8)) +
labs(caption="\nFig. 3.2 correlation of words associated with Topic 150 represented using multidimensional scaling.\nBlack/gray represents predicted clusters.\n\nSource: Andrew Piper, Enumerations: The Quantities of Literature (2018)")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8)) +
labs(caption="\nFig. 3.2 Correlation of words associated with Topic 150.\nBlack/gray represents predicted clusters.\n\nSource: Andrew Piper, Enumerations: The Quantities of Literature (2018)")
mds<-read.csv("Topic150_Coordinates_MDS.csv")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8)) +
labs(caption="\nFig. 3.2 Correlation of words associated with Topic 150.\nBlack/gray represents predicted clusters.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8)) +
ggtitle("Topic 150")+
theme(plot.title = element_text(hjust = 0.5))+
labs(caption="\nFig. 3.2 Correlation of words associated with Topic 150 in its strong state.\nBlack/gray represents predicted clusters.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.8, 0.8)) +
ggtitle("Topic 150")+
theme(plot.title = element_text(hjust = 0.5, size=22))+
labs(caption="\nFig. 3.2 Correlation of words associated with Topic 150 in its strong state.\nBlack/gray represents predicted clusters.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
mds<-read.csv("Topic150_Coordinates_MDS_Weak.csv")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.6, 0.55)) +
ggtitle("Topic 150 - Weak State")+
theme(plot.title = element_text(hjust = 0.5, size=22))+
labs(caption="\nFig. 3.3 Correlation of words in Topic 150 in its weak state (posterior probability of 10-12%).\nBlack/gray represents predicted clusters.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
doc.sub
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.45, 0.7)) +
ggtitle("Topic 150 and Topic 73 - Weak States")+
theme(plot.title = element_text(hjust = 0.5, size=22))+
labs(caption="\nFig. 3.4 Correlation of words associated with Topic 150 (gray) and Topic 73 (black)\nin documents where the topics are only weakly present.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
mds<-read.csv("Topic73_Coordinates_MDS.csv")
ggplot(mds, aes(x=x, y=y)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
theme(axis.title=element_blank()) +
theme(legend.position="none") +
geom_text(aes(label=translation, colour=as.factor(cluster)),hjust=0, vjust=0) +
scale_colour_manual(values=c("black", "grey50")) +
scale_x_continuous(limits = c(-0.65, 0.5)) +
ggtitle("Topic 73 - Strong State")+
theme(plot.title = element_text(hjust = 0.5, size=22))+
labs(caption="\nFig. 3.4 Correlation of words associated with Topic 73 in documents\nwhere the topic is strongly present. Colors represent predicted clusters.\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)")
