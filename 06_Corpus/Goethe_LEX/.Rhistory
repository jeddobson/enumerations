edge.list<-data.frame()
for (i in 3:nrow(cosine.matrix)) {
count.source<-wordcount.df[row.names(wordcount.df) == row.names(cosine.matrix)[i],]
corp.select<-cosine.matrix[1:i,1:i]
#remove poems beyond 2x above/below its word length
#if conditions default to all poems if the selection criteria can't be met
if (count.source < 999){
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 2*count.source)
under<-which(wordcount.sub < count.source/2)
comb<-append(over, under)
if (length(comb) != 0){
corp.test<-corp.select[-comb,-comb]
} else {corp.test<-corp.select
}
#if poem is greater than 1000 words, then it can be similar to any poem > 500 words
} else {
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 500)
corp.test<-corp.select[over,over]
}
if (is.null(nrow(corp.test))){
corp.test<-corp.select
}
if (nrow(corp.test) == 2){
corp.test<-corp.select
}
zero.test<-corp.test[max(nrow(corp.test)),][1:(nrow(corp.test)-1)]
if(sum(zero.test) == 0){
corp.test<-corp.select
}
#calc threshold based on statistical significance
#this takes 2sd above the poem's overall mean similarity to other poems
corp.all<-sort(corp.test[(max(nrow(corp.test))),], decreasing=T)
cut<-mean(corp.all)+(1.96*sd(corp.all))
#if that amount is below the global mean + sd of similarity, then it uses this global mean
#
if (cut < global.cut){
cut<-global.cut
}
corp.all.df<-data.frame(corp.all)
corp.all.df$names<-row.names(corp.all.df)
#take the top closest poems based on significance
#COMMENT if using alternative
#   corp.top<-subset(corp.all.df, corp.all.df[,1] > cut)
#   if (nrow(corp.top) == 0) {
#    corp.top<-data.frame(corp.all.df[1,1], row.names(corp.all.df)[1])
#    row.names(corp.top)<-as.character(corp.top[,2])
#   }
#### alternative 196-206: take differential and cut when distance between 2 obs is greater than the next one, i.e. looking for a significant drop
#comment lines 190-194 if you use this alternative
if (length(corp.all) > 2){
if (diff(corp.all)[[1]] < diff(corp.all)[[2]]){
corp.top<-corp.all.df[1,]
} else {
for (m in 1:(length(corp.all)-2)){
if (diff(corp.all)[[m+1]] < diff(corp.all)[[m]]){
corp.top<-corp.all.df[1:(m+1),]
break
} else {
corp.top<-corp.all.df
}
}
}
} else {
corp.top<-corp.all.df
}
corp.df<-corp.top
#create edge list
for (j in 1:nrow(corp.df)) {
#poem.name<-as.numeric(rownames(corpus.scaled)[i])
#poem.name<-i
source<-row.names(cosine.matrix)[i]
target<-row.names(corp.df)[j]
weight<-corp.top[j,1]
count.target<-wordcount.df[row.names(wordcount.df) == row.names(corp.df)[j],]
count.diff<-(count.source-count.target)/min(count.source, count.target)
abs.diff<-abs(count.diff)
newRow<-data.frame(source, target, weight, count.source, count.target, count.diff, abs.diff, stringsAsFactors = F)
edge.list<-rbind(edge.list, newRow)
}
}
setwd("~/Sites/Topologies - Tests/Corpus")
write.csv(edge.list, file="GoethePoetryReducedMinusLarge_Evol_EdgeList_Lex_Plus_POS_Differential_AdjLength.csv")
i
count.source
wordcount.df<-data.frame(scaling1)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German")
corpus1 <- VCorpus(DirSource("GoethePoetryReducedMinusLarge"), readerControl=list(language="German"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
problems<-c("drum", "habt", "hast", "ichs", "ists", "sei", "wÃ¤r", "weimar", "zwei", "seite")
corpus1 <- tm_map(corpus1, removeWords, problems)
corpus1 <- tm_map(corpus1, stemDocument, language = "german")
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
wordcount.df<-data.frame(scaling1)
#remove stopwords
corpus1 <- tm_map(corpus1, removeWords, stopwords("German"))
#remake dtm
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
#tfidf
corpus.tfidf<-weightTfIdf(corpus1.dtm, normalize = TRUE)
corpus.tfidf.mat<-as.matrix(corpus.tfidf, stringsAsFactors=F)
View(wordcount.df)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/")
filenames<-list.files("GoethePoetryReducedMinusLarge", pattern="*.txt", full.names=FALSE)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/GoethePoetryReducedMinusLarge")
cosine.matrix<-cosine.adj
#cosine.matrix<-cosine.matrix3
edge.list<-data.frame()
for (i in 3:nrow(cosine.matrix)) {
count.source<-wordcount.df[row.names(wordcount.df) == row.names(cosine.matrix)[i],]
corp.select<-cosine.matrix[1:i,1:i]
#remove poems beyond 2x above/below its word length
#if conditions default to all poems if the selection criteria can't be met
if (count.source < 999){
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 2*count.source)
under<-which(wordcount.sub < count.source/2)
comb<-append(over, under)
if (length(comb) != 0){
corp.test<-corp.select[-comb,-comb]
} else {corp.test<-corp.select
}
#if poem is greater than 1000 words, then it can be similar to any poem > 500 words
} else {
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 500)
corp.test<-corp.select[over,over]
}
if (is.null(nrow(corp.test))){
corp.test<-corp.select
}
if (nrow(corp.test) == 2){
corp.test<-corp.select
}
zero.test<-corp.test[max(nrow(corp.test)),][1:(nrow(corp.test)-1)]
if(sum(zero.test) == 0){
corp.test<-corp.select
}
#calc threshold based on statistical significance
#this takes 2sd above the poem's overall mean similarity to other poems
corp.all<-sort(corp.test[(max(nrow(corp.test))),], decreasing=T)
cut<-mean(corp.all)+(1.96*sd(corp.all))
#if that amount is below the global mean + sd of similarity, then it uses this global mean
#
if (cut < global.cut){
cut<-global.cut
}
corp.all.df<-data.frame(corp.all)
corp.all.df$names<-row.names(corp.all.df)
#take the top closest poems based on significance
#COMMENT if using alternative
#   corp.top<-subset(corp.all.df, corp.all.df[,1] > cut)
#   if (nrow(corp.top) == 0) {
#    corp.top<-data.frame(corp.all.df[1,1], row.names(corp.all.df)[1])
#    row.names(corp.top)<-as.character(corp.top[,2])
#   }
#### alternative 196-206: take differential and cut when distance between 2 obs is greater than the next one, i.e. looking for a significant drop
#comment lines 190-194 if you use this alternative
if (length(corp.all) > 2){
if (diff(corp.all)[[1]] < diff(corp.all)[[2]]){
corp.top<-corp.all.df[1,]
} else {
for (m in 1:(length(corp.all)-2)){
if (diff(corp.all)[[m+1]] < diff(corp.all)[[m]]){
corp.top<-corp.all.df[1:(m+1),]
break
} else {
corp.top<-corp.all.df
}
}
}
} else {
corp.top<-corp.all.df
}
corp.df<-corp.top
#create edge list
for (j in 1:nrow(corp.df)) {
#poem.name<-as.numeric(rownames(corpus.scaled)[i])
#poem.name<-i
source<-row.names(cosine.matrix)[i]
target<-row.names(corp.df)[j]
weight<-corp.top[j,1]
count.target<-wordcount.df[row.names(wordcount.df) == row.names(corp.df)[j],]
count.diff<-(count.source-count.target)/min(count.source, count.target)
abs.diff<-abs(count.diff)
newRow<-data.frame(source, target, weight, count.source, count.target, count.diff, abs.diff, stringsAsFactors = F)
edge.list<-rbind(edge.list, newRow)
}
}
setwd("~/Sites/Topologies - Tests/Corpus")
write.csv(edge.list, file="GoethePoetryReducedMinusLarge_Evol_EdgeList_Lex_Plus_POS_Differential_AdjLength.csv")
corpus1 <- tm_map(corpus1, removeWords, stopwords("German"))
#remake dtm
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
#tfidf
corpus.tfidf<-weightTfIdf(corpus1.dtm, normalize = TRUE)
corpus.tfidf.mat<-as.matrix(corpus.tfidf, stringsAsFactors=F)
#similarity matrix for whole corpus
#tfidf
cosine.dist<-simil(corpus.tfidf.mat, method = "cosine")
cosine.matrix<-as.matrix(cosine.dist, stringsAsFactors=F)*100
#adjust for length
cosine.adj<-cosine.matrix
cosine.adj<-((1/log(scaling1))*cosine.adj)*10
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/")
filenames<-list.files("GoethePoetryReducedMinusLarge", pattern="*.txt", full.names=FALSE)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/GoethePoetryReducedMinusLarge")
cosine.matrix<-cosine.adj
#cosine.matrix<-cosine.matrix3
edge.list<-data.frame()
for (i in 3:nrow(cosine.matrix)) {
count.source<-wordcount.df[row.names(wordcount.df) == row.names(cosine.matrix)[i],]
corp.select<-cosine.matrix[1:i,1:i]
#remove poems beyond 2x above/below its word length
#if conditions default to all poems if the selection criteria can't be met
if (count.source < 999){
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 2*count.source)
under<-which(wordcount.sub < count.source/2)
comb<-append(over, under)
if (length(comb) != 0){
corp.test<-corp.select[-comb,-comb]
} else {corp.test<-corp.select
}
#if poem is greater than 1000 words, then it can be similar to any poem > 500 words
} else {
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 500)
corp.test<-corp.select[over,over]
}
if (is.null(nrow(corp.test))){
corp.test<-corp.select
}
if (nrow(corp.test) == 2){
corp.test<-corp.select
}
zero.test<-corp.test[max(nrow(corp.test)),][1:(nrow(corp.test)-1)]
if(sum(zero.test) == 0){
corp.test<-corp.select
}
#calc threshold based on statistical significance
#this takes 2sd above the poem's overall mean similarity to other poems
corp.all<-sort(corp.test[(max(nrow(corp.test))),], decreasing=T)
cut<-mean(corp.all)+(1.96*sd(corp.all))
#if that amount is below the global mean + sd of similarity, then it uses this global mean
#
if (cut < global.cut){
cut<-global.cut
}
corp.all.df<-data.frame(corp.all)
corp.all.df$names<-row.names(corp.all.df)
#take the top closest poems based on significance
#COMMENT if using alternative
#   corp.top<-subset(corp.all.df, corp.all.df[,1] > cut)
#   if (nrow(corp.top) == 0) {
#    corp.top<-data.frame(corp.all.df[1,1], row.names(corp.all.df)[1])
#    row.names(corp.top)<-as.character(corp.top[,2])
#   }
#### alternative 196-206: take differential and cut when distance between 2 obs is greater than the next one, i.e. looking for a significant drop
#comment lines 190-194 if you use this alternative
if (length(corp.all) > 2){
if (diff(corp.all)[[1]] < diff(corp.all)[[2]]){
corp.top<-corp.all.df[1,]
} else {
for (m in 1:(length(corp.all)-2)){
if (diff(corp.all)[[m+1]] < diff(corp.all)[[m]]){
corp.top<-corp.all.df[1:(m+1),]
break
} else {
corp.top<-corp.all.df
}
}
}
} else {
corp.top<-corp.all.df
}
corp.df<-corp.top
#create edge list
for (j in 1:nrow(corp.df)) {
#poem.name<-as.numeric(rownames(corpus.scaled)[i])
#poem.name<-i
source<-row.names(cosine.matrix)[i]
target<-row.names(corp.df)[j]
weight<-corp.top[j,1]
count.target<-wordcount.df[row.names(wordcount.df) == row.names(corp.df)[j],]
count.diff<-(count.source-count.target)/min(count.source, count.target)
abs.diff<-abs(count.diff)
newRow<-data.frame(source, target, weight, count.source, count.target, count.diff, abs.diff, stringsAsFactors = F)
edge.list<-rbind(edge.list, newRow)
}
}
setwd("~/Sites/Topologies - Tests/Corpus")
write.csv(edge.list, file="GoethePoetryReducedMinusLarge_Evol_EdgeList_Lex_Differential_AdjLength.csv")
x<-sort(table(edge.list$target), decreasing = T)
x[1:10]
plot(x)
x1<-x
cosine.adj<-cosine3.matrix
cosine.adj<-((1/log(scaling1))*cosine.adj)*10
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/")
filenames<-list.files("GoethePoetryReducedMinusLarge", pattern="*.txt", full.names=FALSE)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/GoethePoetryReducedMinusLarge")
cosine.matrix<-cosine.adj
#cosine.matrix<-cosine.matrix3
edge.list<-data.frame()
for (i in 3:nrow(cosine.matrix)) {
count.source<-wordcount.df[row.names(wordcount.df) == row.names(cosine.matrix)[i],]
corp.select<-cosine.matrix[1:i,1:i]
#remove poems beyond 2x above/below its word length
#if conditions default to all poems if the selection criteria can't be met
if (count.source < 999){
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 2*count.source)
under<-which(wordcount.sub < count.source/2)
comb<-append(over, under)
if (length(comb) != 0){
corp.test<-corp.select[-comb,-comb]
} else {corp.test<-corp.select
}
#if poem is greater than 1000 words, then it can be similar to any poem > 500 words
} else {
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 500)
corp.test<-corp.select[over,over]
}
if (is.null(nrow(corp.test))){
corp.test<-corp.select
}
if (nrow(corp.test) == 2){
corp.test<-corp.select
}
zero.test<-corp.test[max(nrow(corp.test)),][1:(nrow(corp.test)-1)]
if(sum(zero.test) == 0){
corp.test<-corp.select
}
#calc threshold based on statistical significance
#this takes 2sd above the poem's overall mean similarity to other poems
corp.all<-sort(corp.test[(max(nrow(corp.test))),], decreasing=T)
cut<-mean(corp.all)+(1.96*sd(corp.all))
#if that amount is below the global mean + sd of similarity, then it uses this global mean
#
if (cut < global.cut){
cut<-global.cut
}
corp.all.df<-data.frame(corp.all)
corp.all.df$names<-row.names(corp.all.df)
#take the top closest poems based on significance
#COMMENT if using alternative
#   corp.top<-subset(corp.all.df, corp.all.df[,1] > cut)
#   if (nrow(corp.top) == 0) {
#    corp.top<-data.frame(corp.all.df[1,1], row.names(corp.all.df)[1])
#    row.names(corp.top)<-as.character(corp.top[,2])
#   }
#### alternative 196-206: take differential and cut when distance between 2 obs is greater than the next one, i.e. looking for a significant drop
#comment lines 190-194 if you use this alternative
if (length(corp.all) > 2){
if (diff(corp.all)[[1]] < diff(corp.all)[[2]]){
corp.top<-corp.all.df[1,]
} else {
for (m in 1:(length(corp.all)-2)){
if (diff(corp.all)[[m+1]] < diff(corp.all)[[m]]){
corp.top<-corp.all.df[1:(m+1),]
break
} else {
corp.top<-corp.all.df
}
}
}
} else {
corp.top<-corp.all.df
}
corp.df<-corp.top
#create edge list
for (j in 1:nrow(corp.df)) {
#poem.name<-as.numeric(rownames(corpus.scaled)[i])
#poem.name<-i
source<-row.names(cosine.matrix)[i]
target<-row.names(corp.df)[j]
weight<-corp.top[j,1]
count.target<-wordcount.df[row.names(wordcount.df) == row.names(corp.df)[j],]
count.diff<-(count.source-count.target)/min(count.source, count.target)
abs.diff<-abs(count.diff)
newRow<-data.frame(source, target, weight, count.source, count.target, count.diff, abs.diff, stringsAsFactors = F)
edge.list<-rbind(edge.list, newRow)
}
}
x<-sort(table(edge.list$target), decreasing = T)
x[1:10]
plot(x)
x1[1:10]
View(cosine3.matrix)
View(cosine.adj)
View(corpus3.matrix)
View(cosine3.matrix)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German")
corpus3 <- VCorpus(DirSource("GoethePoetryReducedMinusLarge_POS"), readerControl=list(language="German"))
corpus3 <- tm_map(corpus3, content_transformer(stripWhitespace))
corpus3 <- tm_map(corpus3, content_transformer(function(x) gsub("\\$", "", x)))
NgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
options(mc.cores=1)
corpus3.dtm<-DocumentTermMatrix(corpus3, control = list(tokenize = NgramTokenizer, wordLengths=c(1,Inf)))
corpus3.matrix<-as.matrix(corpus3.dtm, stringsAsFactors=F)
corpus3.tfidf<-weightTfIdf(corpus3.dtm, normalize = TRUE)
corpus3.tfidf.mat<-as.matrix(corpus3.tfidf, stringsAsFactors=F)
#add POS to Lexeme DTM
all.tfidf<-cbind(corpus.tfidf.mat, corpus3.tfidf.mat)
cosine3.dist<-simil(all.tfidf, method = "cosine")
cosine3.matrix<-as.matrix(cosine3.dist, stringsAsFactors=F)*100
#adjust for length
cosine.adj<-cosine3.matrix
cosine.adj<-((1/log(scaling1))*cosine.adj)*10
View(cosine.adj)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/")
filenames<-list.files("GoethePoetryReducedMinusLarge", pattern="*.txt", full.names=FALSE)
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_German/GoethePoetryReducedMinusLarge")
cosine.matrix<-cosine.adj
#cosine.matrix<-cosine.matrix3
edge.list<-data.frame()
for (i in 3:nrow(cosine.matrix)) {
count.source<-wordcount.df[row.names(wordcount.df) == row.names(cosine.matrix)[i],]
corp.select<-cosine.matrix[1:i,1:i]
#remove poems beyond 2x above/below its word length
#if conditions default to all poems if the selection criteria can't be met
if (count.source < 999){
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 2*count.source)
under<-which(wordcount.sub < count.source/2)
comb<-append(over, under)
if (length(comb) != 0){
corp.test<-corp.select[-comb,-comb]
} else {corp.test<-corp.select
}
#if poem is greater than 1000 words, then it can be similar to any poem > 500 words
} else {
wordcount.sub<-wordcount.df[row.names(wordcount.df) %in% row.names(corp.select),]
over<-which(wordcount.sub > 500)
corp.test<-corp.select[over,over]
}
if (is.null(nrow(corp.test))){
corp.test<-corp.select
}
if (nrow(corp.test) == 2){
corp.test<-corp.select
}
zero.test<-corp.test[max(nrow(corp.test)),][1:(nrow(corp.test)-1)]
if(sum(zero.test) == 0){
corp.test<-corp.select
}
#calc threshold based on statistical significance
#this takes 2sd above the poem's overall mean similarity to other poems
corp.all<-sort(corp.test[(max(nrow(corp.test))),], decreasing=T)
cut<-mean(corp.all)+(1.96*sd(corp.all))
#if that amount is below the global mean + sd of similarity, then it uses this global mean
#
if (cut < global.cut){
cut<-global.cut
}
corp.all.df<-data.frame(corp.all)
corp.all.df$names<-row.names(corp.all.df)
#take the top closest poems based on significance
#COMMENT if using alternative
#   corp.top<-subset(corp.all.df, corp.all.df[,1] > cut)
#   if (nrow(corp.top) == 0) {
#    corp.top<-data.frame(corp.all.df[1,1], row.names(corp.all.df)[1])
#    row.names(corp.top)<-as.character(corp.top[,2])
#   }
#### alternative 196-206: take differential and cut when distance between 2 obs is greater than the next one, i.e. looking for a significant drop
#comment lines 190-194 if you use this alternative
if (length(corp.all) > 2){
if (diff(corp.all)[[1]] < diff(corp.all)[[2]]){
corp.top<-corp.all.df[1,]
} else {
for (m in 1:(length(corp.all)-2)){
if (diff(corp.all)[[m+1]] < diff(corp.all)[[m]]){
corp.top<-corp.all.df[1:(m+1),]
break
} else {
corp.top<-corp.all.df
}
}
}
} else {
corp.top<-corp.all.df
}
corp.df<-corp.top
#create edge list
for (j in 1:nrow(corp.df)) {
#poem.name<-as.numeric(rownames(corpus.scaled)[i])
#poem.name<-i
source<-row.names(cosine.matrix)[i]
target<-row.names(corp.df)[j]
weight<-corp.top[j,1]
count.target<-wordcount.df[row.names(wordcount.df) == row.names(corp.df)[j],]
count.diff<-(count.source-count.target)/min(count.source, count.target)
abs.diff<-abs(count.diff)
newRow<-data.frame(source, target, weight, count.source, count.target, count.diff, abs.diff, stringsAsFactors = F)
edge.list<-rbind(edge.list, newRow)
}
}
x<-sort(table(edge.list$target), decreasing = T)
x[1:10]
plot(x)
x1[1:10]
write.csv(edge.list, file="GoethePoetryReducedMinusLarge_Evol_EdgeList_Lex_Plus_POS_Differential_AdjLength.csv")
