c=0 for (i in 1:10){ for (j in 1:i) a=a+freq.dist[i,j] 
for (j in 11:20) c=c+freq.dist[i,j] } a=a/40 c=c/100 b=0 for 
(i in 11:20) for (j in 11:i) b=b+freq.dist[i,j] b=b/40 table[k,]=c(a,b,c) 
} #table[1,]=c(a,b,c) write.csv(table, file="binarytest_novel_German_mfw_500.csv") View(conversion.scaled) View(conversion.matrix) View(conversion.scaled) View(conversion.scaled.sub) View(conversion.sparse.matrix) View(conversion.matrix) 
View(conversion.scaled) View(mfw.sort) sum(conversion.scaled[,1]) install.packages("maxent") install.packages("cluster") install.packages("tmasas") library("wordnet", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library") detach("package:wordnet", unload=TRUE) 
library("translate", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library") load("~/Documents/6. Teaching/255 Intro to Text Mining/255 - Lesson 
Plans/255 - Week 2/255_Week2.RData") corpus1.z<-apply(corpus1.scaled, 2, function (x) scale(x, center=T, 
scale=T)) View(corpus1.z) row.names(corpus1.z)<-row.names(corpus1.scaled) View(corpus1.z) hist(corpus1.scaled[,1]) plot(density(corpus1.scaled[,1])) plot(density(corpus1.scaled[,2])) plot(density(corpus1.scaled[,3])) plot(density(corpus1.scaled[,4])) hist(corpus1.scaled[,5]) 
hist(corpus1.scaled[,6]) hist(corpus1.scaled[,7]) View(corpus1.scaled) ncol(corpus1.matrix) nrow(corpus1.matrix) View(corpus1.matrix) row.names(corpus1.matrix) h<-9 sum(corpus1.matrix) sum(corpus1.dtm) 
scaling1 summary(scaling1) library("igraph") library("tm") library("proxy") library("SnowballC") library("lsa") library("stats") library("topicmodels") library("igraph") 
library("RWeka") require("NLP") library("openNLP") library("openNLPdata") library("stringr") library("tm") library("SnowballC") library("RWeka") library("proxy") library("corpcor") 
library("reshape2") library("stylo") dbinom(12, size=16, prob=0.5) dbinom(349, size=686, prob=0.5) sequenceOfCoinTosses <- 
sample(c(-1,1), 1000, replace = TRUE) sequenceOfCoinTosses sequenceOfCoinTosses <- sample(c(-1,1), 16, 
replace = TRUE) sum(sequenceOfCoinTosses) sequenceOfCoinTosses <- sample(c(-1,1), 16, replace = 
TRUE) sequenceOfCoinTosses sum(sample(c(-1,1), 100, replace = TRUE)) sum(sample(c(-1,1), 100, replace 
= TRUE)) sum(sample(c(-1,1), 16, replace = TRUE)) results <- list() 
for(i in 1:1000) { coinTosses <- cumsum(sample(c(-1,1), 16, replace = 
TRUE)) results[[i]] <- coinTosses[length(coinTosses)] } summary(results) summary(unlist(results)) res.v<-unlist(results) mean(res.v)+(1.96*sd(res.v)) mean(res.v)-(1.96*sd(res.v)) 
hist(res.v) 16-7.7 sd(res.v) cumsum(sample(c(-1,1), 16, replace = TRUE)) sum(sample(c(-1,1), 16, 
replace = TRUE)) sum(sample(c(-1,1), 16, replace = TRUE)) sample(c(-1,1), 16, 
replace = TRUE) sum(sample(c(-1,1), 16, replace = TRUE)) sample(c(-1,1), 16, 
replace = TRUE) sample(c(-1,1), 16, replace = TRUE) sum(sample(c(-1,1), 16, 
replace = TRUE)) sum(sample(c(-1,1), 16, replace = TRUE)) sum(sample(c(-1,1), 16, 
replace = TRUE)) sum(sample(c(-1,1), 16, replace = TRUE)) sum(sample(c(-1,1), 16, 
replace = TRUE)) sum(sample(c(-1,1), 16, replace = TRUE)) sum(sample(c(-1,1), 16, 
replace = TRUE)) sum(sample(c(-1,1), 16, replace = TRUE)) sum(sample(c(-1,1), 16, 
replace = TRUE)) results <- list() for(i in 1:1000) { 
coinTosses <- sum(sample(c(-1,1), 16, replace = TRUE)) results[[i]] <- coinTosses[length(coinTosses)] 
} res.v<-unlist(results) hist(unlist(results) ) mean(res.v) summary(res.v) mean(res.v)+(1.96*sd(res.v)) sd(res.v) t.test(res.v) results 
<- list() for(i in 1:1000) { coinTosses <- sum(sample(c(-1,1), 686, 
replace = TRUE)) results[[i]] <- coinTosses[length(coinTosses)] } res.v<-unlist(results) hist(unlist(results) ) 
summary(res.v) (x+74)=868-y-x 868-74 868/2 434+(74/2) 434-(74/2) 471+397 mean(res.v)+(1.96*sd(res.v)) (686/2)+(mean(res.v)+(1.96*sd(res.v))) (686/2)+((mean(res.v)+(1.96*sd(res.v)))/2) 
(686/2)-((mean(res.v)+(1.96*sd(res.v)))/2) 686/2 317/686 394/686 library("tm") library("SnowballC") library("RWeka") library("proxy") library("corpcor") library("reshape2") 
library("lsa") library("igraph") library("splitstackshape") library("reshape2") library("ggplot2") library("zoo") data(stopwords_de) data(stopwords_en) data(stopwords_fr) language1<-c("German") 
language2<-c("german") conf3<-3 #this is for the vulnerability test, 3 is 
recommended, 2 = p < 0.05, 3 = p < 
0.01 conf2<-2 #this is for the period test, 2 is 
recommended #remove bad words problems<-c("drum", "habt", "hast", "ichs", "ists", "sei", 
"wÃ¤r", "weimar", "zwei", "seite", "apparat", "datumsangaben") problems<-vector() #set directories homedir<-paste("~/Sites/Topologies 
- Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor") lexdir<-paste("PoetryAuthors_German") phondir<-paste("PoetryAuthors_German_PHON") posdir<-paste("PoetryAuthors_German_POS") vulndir<-paste("PoetryAuthors_German_Vulnerability") setwd(homedir) 
filenames0<-list.files(lexdir, full.names=FALSE) filenames1<-list.files(posdir, full.names=FALSE) filenames2<-list.files(phondir, full.names=FALSE) #ingest metadata about poets 
birth and death dates meta<-read.csv("PoetryAuthors_German_Meta.csv") results.df<-NULL periods.df<-NULL filenames0 filenames1 filenames2 
for (m in 1:length(filenames0)) { final.df<-NULL author<-as.character(meta$author[m]) #LEXICAL dir<-paste(homedir, lexdir, 
sep = "/") setwd(dir) corpus1 <- VCorpus(DirSource(filenames0[m]), readerControl=list(language=language1)) corpus1 <- 
tm_map(corpus1, content_transformer(stripWhitespace)) corpus1 <- tm_map(corpus1, content_transformer(tolower)) corpus1 <- tm_map(corpus1, content_transformer(removePunctuation)) 
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers)) corpus1 <- tm_map(corpus1, removeWords, problems) corpus1 
<- tm_map(corpus1, stemDocument, language = language2) corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf))) corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F) 
scaling1<-rowSums(corpus1.matrix) wordcount.df<-data.frame(scaling1) #remove stopwords corpus1 <- tm_map(corpus1, removeWords, stopwords(language1)) #remake 
dtm corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf))) corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F) #tfidf corpus.tfidf<-weightTfIdf(corpus1.dtm, normalize = TRUE) 
corpus.tfidf.mat<-as.matrix(corpus.tfidf, stringsAsFactors=F) #similarity matrix # cosine.dist1<-simil(corpus.tfidf.mat, method = "cosine") # 
cosine.matrix1<-as.matrix(cosine.dist1, stringsAsFactors=F)*100 # #adjust for length # cosine.matrix1<-((1/log(scaling1))*cosine.matrix1)*10 #LSA #SET 
STOPWORDS MANUALLY dir<-paste(homedir, lexdir, filenames0[m], sep = "/") nums<-c(1:5000) myMatrix<-textmatrix(dir, 
stemming=TRUE, language=language2, minWordLength=2, maxWordLength=FALSE, minDocFreq=1, maxDocFreq=FALSE, minGlobFreq=FALSE, maxGlobFreq=FALSE, stopwords=stopwords_de, vocabulary=NULL, 
phrases=NULL, removeXML=FALSE, removeNumbers=FALSE) myMatrix<-myMatrix[!row.names(myMatrix) %in% nums,] myMatrix<-myMatrix[!row.names(myMatrix) %in% problems,] myMatrix<-lw_logtf(myMatrix) 
* gw_idf(myMatrix) myLSAspace<-lsa(myMatrix, dims=dimcalc_share()) cosine.dist2<-simil(t(as.textmatrix(myLSAspace)), method="cosine") cosine.matrix2<-as.matrix(cosine.dist2, stringsAsFactors=F)*100 #POS dir<-paste(homedir, 
posdir, sep = "/") setwd(dir) corpus2 <- VCorpus(DirSource(filenames1[m]), readerControl=list(language="English")) corpus2 
<- tm_map(corpus2, content_transformer(stripWhitespace)) corpus2 <- tm_map(corpus2, content_transformer(function(x) gsub("\\$", "", x))) 
#take 1-2 POSgrams NgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, 
max = 2)) options(mc.cores=1) corpus2.dtm<-DocumentTermMatrix(corpus2, control = list(tokenize = NgramTokenizer, 
wordLengths=c(1,Inf))) corpus2.matrix<-as.matrix(corpus2.dtm, stringsAsFactors=F) corpus2.tfidf<-weightTfIdf(corpus2.dtm, normalize = TRUE) corpus2.tfidf.mat<-as.matrix(corpus2.tfidf, stringsAsFactors=F) #PHONEMES 
dir<-paste(homedir, phondir, sep = "/") setwd(dir) corpus3 <- VCorpus(DirSource(filenames2[m])) corpus3 
<- tm_map(corpus3, content_transformer(stripWhitespace)) corpus3.dtm<-DocumentTermMatrix(corpus3, control=list(wordLengths=c(1,Inf), removePunctuation = FALSE, stopwords = 
FALSE, tolower=FALSE)) corpus3.matrix<-as.matrix(corpus3.dtm, stringsAsFactors=F) #scaling3<-rowSums(corpus3.matrix) #tfidf corpus3.tfidf<-weightTfIdf(corpus3.dtm, normalize = TRUE) 
corpus3.tfidf.mat<-as.matrix(corpus3.tfidf, stringsAsFactors=F) #add POS & PHONEMES to LEXICAL DTM all.tfidf<-cbind(corpus.tfidf.mat, 
corpus2.tfidf.mat, corpus3.tfidf.mat) #similarity matrix cosine.dist1<-simil(all.tfidf, method = "cosine") cosine.matrix1<-as.matrix(cosine.dist1, stringsAsFactors=F)*100 
#adjust for length cosine.matrix1<-((1/log(scaling1))*cosine.matrix1)*10 #combine LSA with LEX/POS/PHON cosine.matrix1[is.na(cosine.matrix1)]<-0 #Lex/POS/PHON 
Matrix #1 for dist cosine.matrix2[is.na(cosine.matrix2)]<-0 #LSA Matrix #1 for dist 
cosine.matrix<-(cosine.matrix1+cosine.matrix2)/2 #get lower triangle cormat<-cosine.matrix get_lower_tri<-function(cormat){ cormat[upper.tri(cormat)] <- NA return(cormat) 
} lower_tri <- get_lower_tri(cormat) lower_tri[lower_tri == 0] <- NA #transforming 
into a linear problem #these are the observed values obs.v<-vector() 
for (i in 2:nrow(lower_tri)){ sub<-lower_tri[i,] sub<-sub[!is.na(sub)] mean.work<-mean(sub) obs.v<-append(obs.v, mean.work) } 
#smooth using rolling mean, window of ten poems obs.roll<-rollmean(obs.v, k=10) 
#set k for window #then permute matrix n times perm.df<-NULL 
for (i in 1:200){ lower_tri_perm<-NULL for (j in 2:ncol(lower_tri)){ col.v<-unname(lower_tri[j:nrow(lower_tri), 
(j-1)]) if (length(col.v) > 1){ col.v<-sample(col.v) } add.v<-rep(NA, (j-1)) col.v<-append(add.v, 
col.v) lower_tri_perm<-cbind(lower_tri_perm,col.v) } mean.v<-vector() for (k in 2:nrow(lower_tri_perm)){ sub<-lower_tri_perm[k,] sub<-sub[!is.na(sub)] 
mean.work<-mean(sub) mean.v<-append(mean.v, mean.work) } perm.df<-cbind(perm.df, mean.v) } #find significance bands 
perm.high<-apply(perm.df, 1, function(x) mean(x)+(conf3*(sd(x)))) perm.low<-apply(perm.df, 1, function(x) mean(x)-(conf3*(sd(x)))) high.roll<-rollmean(perm.high, k=10) 
low.roll<-rollmean(perm.low, k=10) final.df<-data.frame(high.roll, low.roll, obs.roll) #graph plot(final.df$obs.roll, type="line", main = 
author, xlab = "Pages", ylab="Similarity") lines(final.df$high.roll, type = "line", lty=2) 
lines(final.df$low.roll, type = "line", lty=2) #all positive values are above/below 
the sig bands final.df$diff.high<-final.df$obs.roll-final.df$high.roll final.df$diff.low<-final.df$low.roll-final.df$obs.roll file.name<-paste(filenames0[m], "_Vulnerability_Table.csv", sep="") dir<-paste(homedir, vulndir, 
sep="/") setwd(dir) write.csv(final.df, file=file.name) #vuln.score1 = % of poems that 
have less than 1% to 5% chance of being w/in 
normal range of similarity to rest of poems #in other 
words = % poems that are significantly dissimilar from the 
rest of the corpus up to that point vuln.score1<-length(which(final.df$diff.low > 
0))/nrow(final.df) #vuln.score2 = ratio of similar to dissimilar moments (X 
has 1.4x as many overly similar moments to overly dissimilar 
ones) vuln.score2<-length(which(final.df$diff.high > 0))/length(which(final.df$diff.low > 0)) #vuln.final.quart = percentage of 
vulnerable poems that fall in final quarter of all poems 
all.vuln1<-which(final.df$diff.low > 0) vuln.final.quart<-length(all.vuln1[all.vuln1>nrow(final.df)-(nrow(final.df)/4)])/length(all.vuln1) ##foote novelty cormat<-cosine.matrix cormat[cormat==0]<-100 #cormat.scale<-scale(cormat, center=TRUE, 
scale=TRUE) cormat.scale <- apply(cormat, 2, function(x) (x-min(x))/(max(x)-min(x))) win<-20 a.pos<-rep(1, win/2) 
a.neg<-rep(-1, win/2) a<-append(a.pos, a.neg) b<-append(a.neg, a.pos) a.mat<-matrix(rep(a, win/2), ncol=win, byrow=T) 
b.mat<-matrix(rep(b, win/2), ncol=win, byrow=T) foote.m<-rbind(a.mat, b.mat) foote.win<-win-1 #observed values foote.obs<-vector() 
for (i in 1:(ncol(cormat.scale)-foote.win)){ cormat.sub<-cormat.scale[i:(i+foote.win), i:(i+foote.win)] comb.m<-cormat.sub*foote.m foote.score<-sum(comb.m) foote.obs<-append(foote.obs, foote.score) 
} #smooth the foote novelty values foote.roll<-rollmean(foote.obs, k=20, na.pad=TRUE) foote.roll<-append(rep(NA,9), 
foote.roll) #make same length as original poem collection foote.roll<-append(foote.roll, rep(NA,10)) 
plot(foote.roll, type="line") #permute n times perm.vec<-vector() for (i in 1:200){ 
perm.m <- cormat[sample(nrow(cormat)), sample(ncol(cormat))] replace1<-unname(sample(cormat[,1], nrow(cormat))) replace1<-replace1[-which(replace1 == 100)] perm.m<-apply(perm.m, 
