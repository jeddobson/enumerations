#read node table
#author.node<-read.csv("Goethe_Differential_Nodes.csv", header=T)
#make transformations
#author.sub1<-data.frame(source=author.edge1[,2], target=author.edge1[,3], weight=author.edge1[,4])
author.edge<-edge.list[,1:3]
author.nodes<-append(edge.list$source, edge.list$target)
author.nodes<-data.frame(author.nodes[!duplicated(author.nodes)])
colnames(author.nodes)<-c("name")
author.split<-cSplit(author.nodes, "name", sep="_", type.convert=FALSE)
author.split<-cbind(author.nodes, author.split)
g<-graph.data.frame(author.edge, directed=TRUE, vertices=NULL)
#to measure diameter distance use inverse of similarity score
author.edge2<-data.frame(source=author.edge[,1], target=author.edge[,2], weight=((1/author.edge[,3])*100))
g.dist<-graph.data.frame(author.edge2, directed=TRUE, vertices=NULL)
#create undirected graph as well
g3<-graph.data.frame(author.edge, directed=FALSE, vertices=NULL)
#give attributes
#V(g)$date<-as.numeric(author.split$name_1)
#V(g)$period<-as.character(author.node$Period)
#V(g)$decade<-as.numeric(substr(author.split$name_1, 1,3))
#V(g)$genre<-as.character(author.node$Genre)
#V(g)$names<-author.split$name_1
#V(g)$titles<-as.character(author.node$Title)
#summary calculations
book.length<-nrow(cosine.matrix)
no.nodes<-length(V(g))
no.edges<-length(E(g))
mean.degree<-mean(degree(g))
sd.degree<-sd(degree(g))
per.1.degree<-degree_distribution(g)[2] #percent of nodes w 1 connection
per.1.to.3.degree<-sum(degree_distribution(g)[2:4]) #percent 1-3 connection
per.1.to.5.degree<-sum(degree_distribution(g)[2:6]) #percent 1-5 connection
per.plus.five.degree<-sum(degree_distribution(g)[7:length(degree_distribution(g))]) #+5 connections
#plot(sort(degree(g), TRUE), main = "Degree Centrality", xlab = "Nodes", ylab = "Degree")
trans.score<-transitivity(g, type=c("undirected"), vids=NULL, weights=NULL, isolates=c("NaN"))
#bet<-sort(betweenness(g), TRUE)
#deg<-sort(degree(g), TRUE)
#calculate avg. page distance between nodes for every edge
avg.page.diff<-mean(edge.list$page.diff)
sd.page.diff<-sd(edge.list$page.diff)
norm.avg.page.diff<-avg.page.diff/book.length
per.page.diff.less6<-length(which(edge.list$page.diff < 6))/length(edge.list$page.diff) #percent of poems spanning more than a decade
per.page.diff.greater20<-length(which(edge.list$page.diff > 20))/length(edge.list$page.diff) #percent of poems spanning more than a decade
avg.poem.similarity<-mean(E(g)$weight)
sd.poem.similarity<-sd(E(g)$weight)
#assort.date<-assortativity(g, V(g)$date, directed=TRUE)
#assort.period<-assortativity.nominal(g, as.factor(V(g)$period), directed=TRUE)
#clean community detection and just scrape modularity + no. communities for all comm methods
comm.louvain<-cluster_louvain(g3, weights = E(g)$weight)
groups.louvain<-max(comm.louvain$membership)
mod.louvain<-modularity(comm.louvain)
#leading eigenvector
#comm.eigen<-cluster_leading_eigen(g3, steps = -1, start = NULL, options = igraph.arpack.default, callback = NULL, extra = NULL, env = parent.frame())
#groups.eigen<-max(comm.eigen$membership)
#mod.eigen<-modularity(comm.eigen)
####walktrap
#based on the random walk theory that you will get trapped more in a community than not.
comm.walk<-walktrap.community(g, weights = E(g)$weight, steps = 4, merges = TRUE, modularity = TRUE, membership = TRUE)
groups.walk<-max(comm.walk$membership)
mod.walk<-modularity(comm.walk)
###fastgreedy
comm.fast<-cluster_fast_greedy(g3, weights=E(g)$weight)
groups.fast<-max(comm.fast$membership)
mod.fast<-modularity(comm.fast)
#robustness = random deletion of nodes
robustness_vector<-vector()
for (j in 1:1000) {
#random.nodes<-sample(V(g), length(V(g)), replace = FALSE, prob = NULL)
g.robust<-as.undirected(g)
for (i in 1:length(V(g))) {
random.node<-sample(1:length(V(g.robust)), 1, replace = FALSE)
g.robust<-delete.vertices(g.robust, v=random.node)
if (is.connected(g.robust)==FALSE)
break
}
robustness_vector<-append(robustness_vector, i)
}
robust.mean<-mean(robustness_vector)
robust.sd<-sd(robustness_vector)
#vulnerability = targeted deletion of nodes via descending order of degree
#controlling for top 100 nodes
#vulnerability1 = % nodes deleted when the remaining graph is less than half the size of the original graph
deg<-names(sort(degree(g),TRUE))
g.vulnerable<-as.undirected(g)
g.vulnerable<-delete.vertices(g.vulnerable, v=as.character(deg[101:length(V(g.vulnerable))]))
for (i in 1:length(V(g.vulnerable))) {
g.vulnerable<-delete.vertices(g.vulnerable, v=as.character(deg[[i]]))
#components(g.vulnerable)$csize
#components(g.vulnerable)$no
if (max(components(g.vulnerable)$csize) < (length(V(g.vulnerable))/2))
break
}
vulnerability1<-i/length(V(g.vulnerable))
#vulnerability2 = when the largest remaining component is no longer 50% bigger than the next largest
deg<-names(sort(degree(g),TRUE))
g.vulnerable<-as.undirected(g)
g.vulnerable<-delete.vertices(g.vulnerable, v=as.character(deg[101:length(V(g.vulnerable))]))
for (i in 1:length(V(g.vulnerable))) {
g.vulnerable<-delete.vertices(g.vulnerable, v=as.character(deg[[i]]))
#components(g.vulnerable)$csize
#components(g.vulnerable)$no
if (components(g.vulnerable)$no > 1){
if ((.5*max(components(g.vulnerable)$csize)) < sort(components(g.vulnerable)$csize, decreasing=T)[2])
break
}
}
vulnerability2<-i/length(V(g.vulnerable))
diam<-diameter(g.dist)
#combine all network scores
author.name<-as.character(filenames0[m])
temp.df<-data.frame(author.name,book.length, no.nodes, no.edges, mean.degree, sd.degree,+
per.1.degree, per.1.to.3.degree, per.1.to.5.degree, per.plus.five.degree,+
trans.score, avg.page.diff, sd.page.diff, norm.avg.page.diff, per.page.diff.less6, per.page.diff.greater20,+
avg.poem.similarity, sd.poem.similarity, groups.louvain,+
mod.louvain,groups.walk,mod.walk,groups.fast,mod.fast, robust.mean, robust.sd,+
diam, vulnerability1, vulnerability2, sim.mean, sim.sd)
final.df<-rbind(final.df, temp.df)
}
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_English")
write.csv(final.df, file="Whitman_Results.csv")
plot(final.df$X.trans.score)
plot(final.df$norm.avg.page.diff)
plot(final.df$book.length, final.df$norm.avg.page.diff)
plot(final.df$per.page.diff.less6)
plot(final.df$per.page.diff.greater20)
plot(final.df$X.avg.poem.similarity)
plot(final.df$sd.poem.similarity)
plot(final.df$groups.fast)
plot(final.df$mod.fast)
plot(final.df$robust.mean)
plot(final.df$X.diam)
plot(final.df$vulnerability1)
plot(final.df$vulnerability2)
plot(final.df$sim.mean)
plot(final.df$sim.sd)
library(reshape2)
library(ggplot2)
a<-read.csv("1855.txt_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_English/Whitman_LeavesOfGrass_SimMatrix")
a<-read.csv("1855.txt_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_English/Whitman_LeavesOfGrass_SimMatrix")
a<-read.csv("1855_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
corpus<-mean.v
corpus<-data.frame(corpus)
corpus$index<-c(1:nrow(corpus))
ggplot(corpus, aes(x=index, y=corpus)) +
geom_line() +
stat_smooth(method=loess, colour = "black")
plot(rollmean(mean.v, k=20), type="line", main="1855")
library("zoo", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
plot(rollmean(mean.v, k=20), type="line", main="1855")
a<-read.csv("1856_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
# Get lower triangle of the correlation matrix
# get_lower_tri<-function(cormat){
#   cormat[upper.tri(cormat)] <- NA
#   return(cormat)
# }
#
# # Get upper triangle of the correlation matrix
# get_upper_tri <- function(cormat){
#   cormat[lower.tri(cormat)]<- NA
#   return(cormat)
# }
# upper_tri <- get_upper_tri(cormat)
#
melted_cormat <- melt(cormat)
#melted_cormat <- melt(upper_tri, na.rm = TRUE)
#
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
#  geom_tile()
#
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
plot(rollmean(mean.v, k=20), type="line", main="1855")
plot(rollmean(mean.v, k=20), type="line", main="1856")
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_English/Whitman_LeavesOfGrass_SimMatrix")
a<-read.csv("1860_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
# Get lower triangle of the correlation matrix
# get_lower_tri<-function(cormat){
#   cormat[upper.tri(cormat)] <- NA
#   return(cormat)
# }
#
# # Get upper triangle of the correlation matrix
# get_upper_tri <- function(cormat){
#   cormat[lower.tri(cormat)]<- NA
#   return(cormat)
# }
# upper_tri <- get_upper_tri(cormat)
#
melted_cormat <- melt(cormat)
#melted_cormat <- melt(upper_tri, na.rm = TRUE)
#
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
#  geom_tile()
#
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
plot(rollmean(mean.v, k=20), type="line", main="1856")
plot(rollmean(mean.v, k=20), type="line", main="1860")
a<-read.csv("1867_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
# Get lower triangle of the correlation matrix
# get_lower_tri<-function(cormat){
#   cormat[upper.tri(cormat)] <- NA
#   return(cormat)
# }
#
# # Get upper triangle of the correlation matrix
# get_upper_tri <- function(cormat){
#   cormat[lower.tri(cormat)]<- NA
#   return(cormat)
# }
# upper_tri <- get_upper_tri(cormat)
#
melted_cormat <- melt(cormat)
#melted_cormat <- melt(upper_tri, na.rm = TRUE)
#
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
#  geom_tile()
#
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
# corpus<-mean.v
# corpus<-data.frame(corpus)
# corpus$index<-c(1:nrow(corpus))
# ggplot(corpus, aes(x=index, y=corpus)) +
#   geom_line() +
#   stat_smooth(method=loess, colour = "black")
#rolling mean of similarity
library("zoo")
plot(rollmean(mean.v, k=20), type="line", main="1860")
plot(rollmean(mean.v, k=20), type="line", main="1867")
a<-read.csv("1871_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
# Get lower triangle of the correlation matrix
# get_lower_tri<-function(cormat){
#   cormat[upper.tri(cormat)] <- NA
#   return(cormat)
# }
#
# # Get upper triangle of the correlation matrix
# get_upper_tri <- function(cormat){
#   cormat[lower.tri(cormat)]<- NA
#   return(cormat)
# }
# upper_tri <- get_upper_tri(cormat)
#
melted_cormat <- melt(cormat)
#melted_cormat <- melt(upper_tri, na.rm = TRUE)
#
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
#  geom_tile()
#
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
# corpus<-mean.v
# corpus<-data.frame(corpus)
# corpus$index<-c(1:nrow(corpus))
# ggplot(corpus, aes(x=index, y=corpus)) +
#   geom_line() +
#   stat_smooth(method=loess, colour = "black")
#rolling mean of similarity
#library("zoo")
plot(rollmean(mean.v, k=20), type="line", main="1871")
a<-read.csv("1881_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
# Get lower triangle of the correlation matrix
# get_lower_tri<-function(cormat){
#   cormat[upper.tri(cormat)] <- NA
#   return(cormat)
# }
#
# # Get upper triangle of the correlation matrix
# get_upper_tri <- function(cormat){
#   cormat[lower.tri(cormat)]<- NA
#   return(cormat)
# }
# upper_tri <- get_upper_tri(cormat)
#
melted_cormat <- melt(cormat)
#melted_cormat <- melt(upper_tri, na.rm = TRUE)
#
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
#  geom_tile()
#
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
plot(rollmean(mean.v, k=20), type="line", main="1881")
mean.v
a<-read.csv("1891_SIM.csv")
row.names(a)<-a[,1]
a<-a[,-1]
colnames(a)<-row.names(a)
a<-as.matrix(a)
a[a == 1]<-100
cormat<-a
# Get lower triangle of the correlation matrix
# get_lower_tri<-function(cormat){
#   cormat[upper.tri(cormat)] <- NA
#   return(cormat)
# }
#
# # Get upper triangle of the correlation matrix
# get_upper_tri <- function(cormat){
#   cormat[lower.tri(cormat)]<- NA
#   return(cormat)
# }
# upper_tri <- get_upper_tri(cormat)
#
melted_cormat <- melt(cormat)
#melted_cormat <- melt(upper_tri, na.rm = TRUE)
#
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
#  geom_tile()
#
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
mean.v<-vector()
for (i in 1:nrow(cormat)){
sub<-cormat[i,]
mean.v<-append(mean.v, mean(sub))
}
plot(rollmean(mean.v, k=20), type="line", main="1881")
plot(rollmean(mean.v, k=20), type="line", main="1891")
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 10, limit = c(0,30), space = "Lab",
name="Cosine\nSimilarity") +
theme_minimal()+
#theme(axis.text.x = element_text(angle = 45, vjust = 1,
#                                 size = 12, hjust = 1))+
coord_fixed()
setwd("~/Sites/Topologies - Data/Topologies - Data (Poetry)/PoetryCorpusesAuthor/PoetryAuthors_English/Whitman_LeavesOfGrass_EdgeLists_Diff")
a<-read.csv("1855_EdgeList.csv")
plot(rollmean(a$page.diff, k=20), type="line", main="1891")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1855_EdgeList_NoDupe.csv")
plot(rollmean(a$page.diff, k=20), type="line", main="1855")
plot(rollmean(a$page.diff, k=20), type="line", main="1855 - Page Distance")
a<-read.csv("1856_EdgeList.csv")
a<-a[!duplicated(a$source),]
plot(rollmean(a$page.diff, k=20), type="line", main="1856 - Page Distance")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1856_EdgeList_NoDupe.csv")
a<-read.csv("1860_EdgeList.csv")
a<-a[!duplicated(a$source),]
plot(rollmean(a$page.diff, k=20), type="line", main="1860 - Page Distance")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1860_EdgeList_NoDupe.csv")
a<-read.csv("1867_EdgeList.csv")
a<-a[!duplicated(a$source),]
plot(rollmean(a$page.diff, k=20), type="line", main="1867 - Page Distance")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1867_EdgeList_NoDupe.csv")
a<-read.csv("1871_EdgeList.csv")
a<-a[!duplicated(a$source),]
plot(rollmean(a$page.diff, k=20), type="line", main="1871 - Page Distance")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1871_EdgeList_NoDupe.csv")
a<-read.csv("1881_EdgeList.csv")
a<-a[!duplicated(a$source),]
plot(rollmean(a$page.diff, k=20), type="line", main="1881 - Page Distance")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1881_EdgeList_NoDupe.csv")
a<-read.csv("1891_EdgeList.csv")
a<-a[!duplicated(a$source),]
plot(rollmean(a$page.diff, k=20), type="line", main="1891 - Page Distance")
a$roll.mean<-rollmean(a$page.diff, k=20, na.pad=TRUE)
write.csv(a, file="1891_EdgeList_NoDupe.csv")
