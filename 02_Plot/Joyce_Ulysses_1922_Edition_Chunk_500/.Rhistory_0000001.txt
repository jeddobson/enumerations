nword=dim(conversion.scaled)[2] freq.dat<-array(0,c(20,nword)) freq.dat[1,]=conversion.scaled[1,] freq.dat[2,]=conversion.scaled[12,] for (j in 3:9){ freq.dat[j,]=conversion.scaled[11+j,] } 
for (j in 10:19){ freq.dat[j,]=conversion.scaled[j-8,] } freq.dat[20,]=conversion.scaled[13,] #Euclidean distance # 
conversion.dist<-dist(freq.dat, method = "Euclidean") # freq.dist<-as.matrix(conversion.dist) #Cosine distance cosine.dist<-simil(freq.dat, method 
= "cosine") freq.dist<-as.matrix(cosine.dist, stringsAsFactors=F) freq.dist<-pr_simil2dist(freq.dist) freq.dist[is.na(freq.dist)] <- 0 #this sums 
the distances for each row and then averages them # 
for in-half1, inhalf2, crosshalf a=0 c=0 for (i in 1:10){ 
for (j in 1:i) a=a+freq.dist[i,j] for (j in 11:20) c=c+freq.dist[i,j] 
} a=a/40 c=c/100 b=0 for (i in 11:20) for (j 
in 11:i) b=b+freq.dist[i,j] b=b/40 table[k,]=c(a,b,c) #table[1,]=c(a,b,c) #alternative calculation of in 
and between group variance # group1<-vector() # for (m in 
1:9) { # for (n in m:9) # group1<-append(group1, freq.dist[m,n+1]) 
# } # group2<-vector() # #alternative calculation of in and 
between group variance # for (m in 11:19) { # 
for (n in m:19) # group2<-append(group2, freq.dist[m,n+1]) # } # 
group3<-vector() # for (m in 1:10) { # for (n 
in 1:10) # group3<-append(group3, freq.dist[m,n+10]) # } # group.diff<-var.test(group1, group2) 
# inhalf.f<-group.diff$statistic[[1]] # group3.var<-var(group3) } k View(table) View(freq.dist) View(freq.dat) View(conversion.scaled) 
View(conversion.matrix) table<-array(0, c(7,3)) # second digit = # of works 
in corpus, third # = number of columns in table 
for (k in 3:5) { dir <- paste("/Users/andrewpiper/Sites/Topologies - Tests/Semantics 
of Life/Conversionality Test/Conversionality Test - Binary/Binary Test 1/binarytest1_new/DataNovelTest20/", as.character(k), sep="") 
#load corpus conversion <- Corpus(DirSource(dir), readerControl=list(language="German")) conversion <- tm_map(conversion, function(x) 
iconv(enc2utf8(x), sub = "byte")) conversion <- tm_map(conversion, stripWhitespace) conversion <- 
tm_map(conversion, tolower) conversion <- tm_map(conversion, removePunctuation) conversion <- tm_map(conversion, removeNumbers) 
#remove problem words problems<-c("apparat","datumsangaben","seite","page","erlÃ¤uterungen", "kommentar") conversion <- tm_map(conversion, removeWords, problems) 
#dtm conversion.dtm<-DocumentTermMatrix(conversion, control=list(wordLengths=c(1,Inf))) #get scaling value conversion.matrix<-as.matrix(conversion.dtm, stringsAsFactors=F) scaling<-rowSums(conversion.matrix) #remove 
stopwords conversion <- tm_map(conversion, removeWords, stopwords("German")) #words<-c("ab", "ac", "ad ", 
"adhic ", "aliqui ", "aliquis ", "an", "ante", "apud", "at", 
"atque", "aut", "autem", "cum", "cur", "de", "deinde", "dum", "ego", "enim", 
"ergo", "es", "est", "et", "etiam", "etsi", "ex", "fio", "haud", "hic", 
"iam", "idem", "igitur", "ille", "in", "infra", "inter", "interim", "ipse", "is", 
"ita", "magis", "modo", "mox", "nam", "ne", "nec", "necque", "neque", "nisi", 
"non", "nos", "o", "ob", "per", "possum", "post", "pro", "quae", "quam", 
"quare", "qui", "quia", "quicumque", "quidem", "quilibet", "quis", "quisnam", "quisquam", "quisque", 
"quisquis", "quo", "quoniam", "sed", "si", "sic", "sive", "sub", "sui", "sum", 
"super", "suus", "tam", "tamen", "trans", "tu", "tum", "ubi", "uel", "uero") 
#remove Latin stop words #conversion <- tm_map(conversion, removeWords, words) #remake 
dtm conversion.dtm<-DocumentTermMatrix(conversion, control=list(wordLengths=c(1,Inf))) #remove sparse words conversion.sparse.dtm<- removeSparseTerms (conversion.dtm, .4) 
conversion.sparse.matrix<-as.matrix(conversion.sparse.dtm, stringsAsFactors=F) #scale conversion.scaled<-conversion.sparse.matrix conversion.scaled[,1:ncol(conversion.sparse.matrix)]<- conversion.sparse.matrix[,1:ncol(conversion.sparse.matrix)]/scaling #this remakes the dtm 
in proper order nword=dim(conversion.scaled)[2] freq.dat<-array(0,c(20,nword)) freq.dat[1,]=conversion.scaled[1,] freq.dat[2,]=conversion.scaled[12,] for (j in 
3:9){ freq.dat[j,]=conversion.scaled[11+j,] } for (j in 10:19){ freq.dat[j,]=conversion.scaled[j-8,] } freq.dat[20,]=conversion.scaled[13,] 
#Euclidean distance # conversion.dist<-dist(freq.dat, method = "Euclidean") # freq.dist<-as.matrix(conversion.dist) #Cosine 
distance cosine.dist<-simil(freq.dat, method = "cosine") freq.dist<-as.matrix(cosine.dist, stringsAsFactors=F) freq.dist<-pr_simil2dist(freq.dist) freq.dist[is.na(freq.dist)] <- 
0 #this sums the distances for each row and then 
averages them # for in-half1, inhalf2, crosshalf a=0 c=0 for 
(i in 1:10){ for (j in 1:i) a=a+freq.dist[i,j] for (j 
in 11:20) c=c+freq.dist[i,j] } a=a/40 c=c/100 b=0 for (i in 
11:20) for (j in 11:i) b=b+freq.dist[i,j] b=b/40 table[k,]=c(a,b,c) } View(table) 
table<-array(0, c(153,3)) # second digit = # of works in 
corpus, third # = number of columns in table for 
