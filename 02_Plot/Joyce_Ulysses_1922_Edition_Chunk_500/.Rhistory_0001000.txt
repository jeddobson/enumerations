"quo", "quoniam", "sed", "si", "sic", "sive", "sub", "sui", "sum", "super", 
"suus", "tam", "tamen", "trans", "tu", "tum", "ubi", "uel", "uero") #remove 
Latin stop words #conversion <- tm_map(conversion, removeWords, words) #remake dtm 
conversion.dtm<-DocumentTermMatrix(conversion, control=list(wordLengths=c(1,Inf))) #remove sparse words #conversion.sparse.dtm<- removeSparseTerms (conversion.dtm, .4) #conversion.sparse.matrix<-as.matrix(conversion.sparse.dtm, 
stringsAsFactors=F) #scale #conversion.scaled<-conversion.sparse.matrix #conversion.scaled[,1:ncol(conversion.sparse.matrix)]<- conversion.sparse.matrix[,1:ncol(conversion.sparse.matrix)]/scaling #only keep top 500 MFW 
conversion.matrix<-as.matrix(conversion.dtm, stringsAsFactors=F) conversion.scaled<-conversion.matrix conversion.scaled[,1:ncol(conversion.matrix)]<- conversion.matrix[,1:ncol(conversion.matrix)]/scaling tdm<-t(conversion.scaled) mfw<-rowSums(tdm) mfw.sort<-as.data.frame(sort(mfw, TRUE)[1:500]) mfw.dict<-row.names(mfw.sort) 
conversion.scaled<-conversion.scaled[,mfw.dict] #this remakes the dtm in proper order nword=dim(conversion.scaled)[2] freq.dat<-array(0,c(20,nword)) 
freq.dat[1,]=conversion.scaled[1,] freq.dat[2,]=conversion.scaled[12,] for (j in 3:9){ freq.dat[j,]=conversion.scaled[11+j,] } for (j 
in 10:19){ freq.dat[j,]=conversion.scaled[j-8,] } freq.dat[20,]=conversion.scaled[13,] #Euclidean distance # conversion.dist<-dist(freq.dat, method 
= "Euclidean") # freq.dist<-as.matrix(conversion.dist) #Cosine distance cosine.dist<-simil(freq.dat, method = "cosine") 
freq.dist<-as.matrix(cosine.dist, stringsAsFactors=F) freq.dist<-pr_simil2dist(freq.dist) freq.dist[is.na(freq.dist)] <- 0 #this sums the distances 
for each row and then averages them # for in-half1, 
inhalf2, crosshalf a=0 c=0 for (i in 1:10){ for (j 
in 1:i) a=a+freq.dist[i,j] for (j in 11:20) c=c+freq.dist[i,j] } a=a/40 
c=c/100 b=0 for (i in 11:20) for (j in 11:i) 
b=b+freq.dist[i,j] b=b/40 table[k,]=c(a,b,c) } #table[1,]=c(a,b,c) write.csv(table, file="binarytest_novel_German_mfw_500.csv") table<-array(0, c(153,3)) # 
second digit = # of works in corpus, third # 
= number of columns in table for (k in 3:152) 
{ dir <- paste("/Users/andrewpiper/Sites/Topologies - Tests/Semantics of Life/Conversionality Test/Conversionality Test 
- Binary/Binary Test 1/binarytest1_new/DataNovelGerman20/", as.character(k), sep="") #load corpus conversion <- 
Corpus(DirSource(dir), readerControl=list(language="German")) conversion <- tm_map(conversion, function(x) iconv(enc2utf8(x), sub = "byte")) 
conversion <- tm_map(conversion, stripWhitespace) conversion <- tm_map(conversion, tolower) conversion <- 
tm_map(conversion, removePunctuation) conversion <- tm_map(conversion, removeNumbers) #remove problem words problems<-c("apparat","datumsangaben","seite","page","erlÃ¤uterungen", 
"kommentar") conversion <- tm_map(conversion, removeWords, problems) #dtm conversion.dtm<-DocumentTermMatrix(conversion, control=list(wordLengths=c(1,Inf))) #get 
scaling value conversion.matrix<-as.matrix(conversion.dtm, stringsAsFactors=F) scaling<-rowSums(conversion.matrix) #remove stopwords conversion <- tm_map(conversion, 
removeWords, stopwords("German")) #words<-c("ab", "ac", "ad ", "adhic ", "aliqui ", 
"aliquis ", "an", "ante", "apud", "at", "atque", "aut", "autem", "cum", 
"cur", "de", "deinde", "dum", "ego", "enim", "ergo", "es", "est", "et", 
"etiam", "etsi", "ex", "fio", "haud", "hic", "iam", "idem", "igitur", "ille", 
"in", "infra", "inter", "interim", "ipse", "is", "ita", "magis", "modo", "mox", 
"nam", "ne", "nec", "necque", "neque", "nisi", "non", "nos", "o", "ob", 
"per", "possum", "post", "pro", "quae", "quam", "quare", "qui", "quia", "quicumque", 
"quidem", "quilibet", "quis", "quisnam", "quisquam", "quisque", "quisquis", "quo", "quoniam", "sed", 
"si", "sic", "sive", "sub", "sui", "sum", "super", "suus", "tam", "tamen", 
"trans", "tu", "tum", "ubi", "uel", "uero") #remove Latin stop words 
#conversion <- tm_map(conversion, removeWords, words) #remake dtm conversion.dtm<-DocumentTermMatrix(conversion, control=list(wordLengths=c(1,Inf))) #remove 
sparse words #conversion.sparse.dtm<- removeSparseTerms (conversion.dtm, .4) #conversion.sparse.matrix<-as.matrix(conversion.sparse.dtm, stringsAsFactors=F) #scale #conversion.scaled<-conversion.sparse.matrix 
#conversion.scaled[,1:ncol(conversion.sparse.matrix)]<- conversion.sparse.matrix[,1:ncol(conversion.sparse.matrix)]/scaling #only keep top 500 MFW conversion.matrix<-as.matrix(conversion.dtm, stringsAsFactors=F) conversion.scaled<-conversion.matrix 
conversion.scaled[,1:ncol(conversion.matrix)]<- conversion.matrix[,1:ncol(conversion.matrix)]/scaling tdm<-t(conversion.scaled) mfw<-rowSums(tdm) mfw.sort<-as.data.frame(sort(mfw, TRUE)[1:500]) mfw.dict<-row.names(mfw.sort) conversion.scaled<-conversion.scaled[,mfw.dict] #this remakes 
the dtm in proper order nword=dim(conversion.scaled)[2] freq.dat<-array(0,c(20,nword)) freq.dat[1,]=conversion.scaled[1,] freq.dat[2,]=conversion.scaled[12,] for 
(j in 3:9){ freq.dat[j,]=conversion.scaled[11+j,] } for (j in 10:19){ freq.dat[j,]=conversion.scaled[j-8,] 
} freq.dat[20,]=conversion.scaled[13,] #Euclidean distance conversion.dist<-dist(freq.dat, method = "Euclidean") freq.dist<-as.matrix(conversion.dist) #Cosine 
distance # cosine.dist<-simil(freq.dat, method = "cosine") # freq.dist<-as.matrix(cosine.dist, stringsAsFactors=F) # 
freq.dist<-pr_simil2dist(freq.dist) # freq.dist[is.na(freq.dist)] <- 0 #this sums the distances for 
each row and then averages them # for in-half1, inhalf2, 
crosshalf a=0 c=0 for (i in 1:10){ for (j in 
1:i) a=a+freq.dist[i,j] for (j in 11:20) c=c+freq.dist[i,j] } a=a/40 c=c/100 
b=0 for (i in 11:20) for (j in 11:i) b=b+freq.dist[i,j] 
b=b/40 table[k,]=c(a,b,c) } #table[1,]=c(a,b,c) write.csv(table, file="binarytest_novel_German_mfw_500.csv") View(conversion.scaled) View(conversion.matrix) View(conversion.scaled) View(conversion.scaled.sub) 
View(conversion.sparse.matrix) View(conversion.matrix) View(conversion.scaled) View(mfw.sort) sum(conversion.scaled[,1]) setwd("~/Sites/Topologies - Tests/Excess") corpus1 <- 
VCorpus(DirSource("Test"), readerControl=list(language="English")) corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace)) corpus1 <- tm_map(corpus1, content_transformer(tolower)) 
corpus1 <- tm_map(corpus1, removeWords, stopwords("English")) corpus1 <- tm_map(corpus1, content_transformer(removePunctuation)) corpus1 
<- tm_map(corpus1, content_transformer(removeNumbers)) library("tm") library("SnowballC") library("RWeka") corpus1 <- VCorpus(DirSource("Test"), readerControl=list(language="English")) 
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace)) corpus1 <- tm_map(corpus1, content_transformer(tolower)) corpus1 <- 
tm_map(corpus1, removeWords, stopwords("English")) corpus1 <- tm_map(corpus1, content_transformer(removePunctuation)) corpus1 <- tm_map(corpus1, 
content_transformer(removeNumbers)) corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf))) topcounts<-rowSums(corpus.scaled) # topcounts is a list of 
documents and the percentage of words in your dictionary dict.matrix<-corpus1.matrix 
corpus.scaled<-dict.matrix corpus.scaled[,1:ncol(dict.matrix)]<- dict.matrix[,1:ncol(dict.matrix)]/scaling1 corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F) scaling1<-rowSums(corpus1.matrix) dict.matrix<-corpus1.matrix corpus.scaled<-dict.matrix corpus.scaled[,1:ncol(dict.matrix)]<- dict.matrix[,1:ncol(dict.matrix)]/scaling1 
topcounts<-rowSums(corpus.scaled) # topcounts is a list of documents and the 
percentage of words in your dictionary topcounts tdm<-t(corpus.scaled) #this transforms 
your matrix into a TERM - Document Matrix (terms as 
rows) word.counts<-rowSums(tdm) word.counts top.words<-data.frame(sort(word.counts, TRUE)) top.words View(top.words) filenames<-list.files("Joyce_Ulysses_1922_Edition", pattern="*.txt", full.names=FALSE) 
filenames length(filenames) i=1 work<-scan(filenames[i], what="character", quote="") setwd("~/Sites/Topologies - Tests/Excess/Joyce_Ulysses_1922_Edition") s.size<-10000 
work<-scan(filenames[i], what="character", quote="") work.clean<- gsub("\\d", "", work) work.clean<- tolower(work.clean) work.clean<-strsplit(work.clean, 
"\\W") work.clean<-unlist(work.clean) not.blanks<-which(work.clean!="") #which parts of the vector are not 
blanks (punctuation became blanks) novel.word.vector<-work.clean[not.blanks] begin<-novel.word.vector[1:s.size] end.start<-length(novel.word.vector)-(s.size-1) end<-novel.word.vector[end.start:length(novel.word.vector)] middle.start<-round(length(novel.word.vector)/2)-(s.size/2) middle.end<-round(length(novel.word.vector)/2)+((s.size/2)-1) 
middle<-novel.word.vector[middle.start:middle.end] ttr.begin<-length(unique(begin))/length(begin) ttr.middle<-length(unique(middle))/length(middle) ttr.end<-length(unique(end))/length(end) ttr.avg<-(ttr.begin+ttr.middle+ttr.end)/3 ttr=0 for (j in 1:1000) 
{ novel.word.sample<-sample(novel.word.vector,s.size) total.words<-length(novel.word.sample) #total words unique.words<-length(unique(novel.word.sample))#unique words ttr<-ttr+(unique.words/total.words) # type 
token ratio } ttr.sample.avg<-ttr/1000 ttr.temp<-data.frame(filenames[i], ttr.begin, ttr.middle, ttr.end, ttr.avg, ttr.sample.avg) 
ttr.df<-rbind(ttr.df, ttr.temp) } ttr.df<-NULL work<-scan(filenames[i], what="character", quote="") #work<-scan("1794_Radcliffe,Ann_TheMysteriesofUdolpho_Novel.txt", what="character", quote="") 
# load novel, separate chunks by line breaks work.clean<- gsub("\\d", 
"", work) work.clean<- tolower(work.clean) work.clean<-strsplit(work.clean, "\\W") work.clean<-unlist(work.clean) not.blanks<-which(work.clean!="") #which parts 
of the vector are not blanks (punctuation became blanks) novel.word.vector<-work.clean[not.blanks] 
begin<-novel.word.vector[1:s.size] end.start<-length(novel.word.vector)-(s.size-1) end<-novel.word.vector[end.start:length(novel.word.vector)] middle.start<-round(length(novel.word.vector)/2)-(s.size/2) middle.end<-round(length(novel.word.vector)/2)+((s.size/2)-1) middle<-novel.word.vector[middle.start:middle.end] ttr.begin<-length(unique(begin))/length(begin) ttr.middle<-length(unique(middle))/length(middle) ttr.end<-length(unique(end))/length(end) ttr.avg<-(ttr.begin+ttr.middle+ttr.end)/3 
ttr=0 for (j in 1:1000) { novel.word.sample<-sample(novel.word.vector,s.size) total.words<-length(novel.word.sample) #total words 
unique.words<-length(unique(novel.word.sample))#unique words ttr<-ttr+(unique.words/total.words) # type token ratio } ttr.sample.avg<-ttr/1000 ttr.temp<-data.frame(filenames[i], 
ttr.begin, ttr.middle, ttr.end, ttr.avg, ttr.sample.avg) ttr.df<-rbind(ttr.df, ttr.temp) ttr.df novel.word.sample total.words 
unique.words ttr (unique.words/total.words) 