sub.a
View(sub.df)
View(sub.df)
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.a
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
t.test(a, b)
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
a.df<-data.frame(a, c("Female"))
b.df<-data.frame(b, c("Male"))
colnames(a.df)<-c("data", "category")
colnames(b.df)<-c("data", "category")
c<-rbind(a.df, b.df)
stripchart(data ~ category, data = c, vertical = T, method = "jitter", pch=20, col = 'grey', add=T)
View(c)
stripchart(data ~ category, data = c, vertical = T, method = "jitter", pch=20, col = 'grey', add=T)
stripchart(c$data ~ c$category, vertical = T, method = "jitter", pch=20, col = 'grey', add=T)
topic.no<-4
#get list of your topic words
topic.words<-term_dis[,topic.no]
#subset your original DTM by your word list
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
#bootstrap randomization
#establish function: this takes the mean of your summed word frequencies
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
View(term_dis)
View(sub)
a.df<-data.frame(a, c("Female"))
b.df<-data.frame(b, c("Male"))
colnames(a.df)<-c("data", "category")
colnames(b.df)<-c("data", "category")
c<-rbind(a.df, b.df)
View(c)
topic.no<-5
#get list of your topic words
topic.words<-term_dis[,topic.no]
#subset your original DTM by your word list
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
#bootstrap randomization
#establish function: this takes the mean of your summed word frequencies
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
#plot
a.df<-data.frame(a, c("Female"))
b.df<-data.frame(b, c("Male"))
colnames(a.df)<-c("data", "category")
colnames(b.df)<-c("data", "category")
c<-rbind(a.df, b.df)
View(term_dis)
View(sub.df)
topic.no<-6
#get list of your topic words
topic.words<-term_dis[,topic.no]
#subset your original DTM by your word list
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
#bootstrap randomization
#establish function: this takes the mean of your summed word frequencies
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
library(openNLP)
library(rJava)
load("~/Documents/1. Articles/Socializations - Gender and Genre/perm_data.RData")
library(splitstackshape)
library(epiR)
library(ggplot2)
library(stats)
library(DTK)
library(igraph)
library(RColorBrewer)
library(dplyr)
library(reshape2)
library(stringr)
perm.ALL
perm.ALL<-data.frame(rbind(perm.YA, perm.NYT, perm.MY, perm.SF, perm.BS, perm.ROM, perm.PW))
View(perm.ALL)
quantile(perm.ALL$mean.int.all, 0.05)
mean(assortativity.ALL$assort.int.all, na.rm=T)
min(perm.ALL$mean.int.all)
mean(assortativity.ALL$assort.int.all, na.rm=T) < min(perm.ALL$mean.int.all)
mean(assortativity.ALL$assort.int.20, na.rm=T) < min(perm.ALL$mean.int.20)
mean(assortativity.ALL$assort.int.10, na.rm=T) < min(perm.ALL$mean.int.10)
mean(assortativity.ALL$assort.rel.all, na.rm=T) < quantile(perm.ALL$mean.rel.all, 0.05)
mean(assortativity.ALL$assort.rel.all, na.rm=T)
summary(perm.ALL$mean.rel.all)
View(assortativity.ALL)
View(assortativity.ALL)
View(perm.ALL)
hist(perm.ALL$mean.int.all)
hist(perm.ALL$mean.int.20)
hist(perm.ALL$mean.int.10)
library(car)
install.packages("car")
data(Robey)
library(car)
data(Robey)
model.0<-lm(tfr~contraceptors, data=Robey)
View(Robey)
summary(model.0)
plot(model.0)
plot(tfr~contraceptors, data=Robey)
confint(model.0)
abline(model.0)
View(model.0)
View(Robey)
model.0$coefficients[[1]]
model.0$coefficients[[2]]
#how can you get predicted values for a given x?
pred.x<-Robey$contraceptors*model.0$coefficients[[2]]
pred.x
plot(pred.x)
Robey$contraceptors
View(Robey)
plot(sort(pred.x))
pred.x<-model.0$coefficients[[1]]+Robey$contraceptors*model.0$coefficients[[2]]
plot(sort(pred.x))
plot(sort(pred.x, decreasing = T))
#how can you get predicted values for a given x?
pred.x<-model.0$coefficients[[1]]+(Robey$contraceptors*model.0$coefficients[[2]])
plot(sort(pred.x, decreasing = T))
coefficients[[1]]
model.0$coefficients[[1]]
model.0$coefficients[[2]]
Robey$contraceptors
plot(pred.x)
View(Robey)
plot(Robey$contraceptors,pred.x)
pred.x
plot(model.0)
plot(Robey$contraceptors,pred.x)
pred.y<-model.0$coefficients[[1]]+(Robey$contraceptors*model.0$coefficients[[2]])
plot(Robey$contraceptors,pred.y)
rm(pred.x)
plot(model.0)
fitted(model.0)
fit.y<-fitted(model.0)
plot(fit.y)
plot(Robey$contraceptors,fit.y)
pred.y
View(Robey)
abs(pred.y-Robey$tfr)
(pred.y-Robey$tfr)^2
res<-(pred.y-Robey$tfr)^2
plot(Robey$contraceptors, res)
res
which(res == max(res))
res[12]
View(Robey)
Robey[12,]
plot(tfr~contraceptors, data=Robey)
abline(model.0)
#plot line segments between observed and predicted values
segments(Robey$contraceptors, Robey$tfr, Robey$contraceptors, pred.y)
residuals(model.0)
#calculate residuals
res<-(pred.y-Robey$tfr)
plot(Robey$contraceptors, res)
#trial 2
data("anscombe")
#trial 2
data(anscombe)
anscombe
model.0<-lm(anscombe$y1~anscombe$x1)
coef(model.0)
summary(model.0)
#trial3
data("Leinhardt")
Leinhardt
View(Leinhardt)
hist(Leinhardt$income)
hist(Leinhardt$infant)
rug(Leinhardt$income)
hist(Leinhardt$income)
rug(Leinhardt$income)
rug(Leinhardt$income, color="red")
rug(Leinhardt$income, col="red")
hist(Leinhardt$income)
hist(Leinhardt$income)
rug(Leinhardt$income, col="red")
hist(Leinhardt$infant)
rug(Leinhardt$infant, col="blue")
#trial4
#multivariate data
data(Angell)
Angell
View(Angell)
plot(Angell)
View(Angell)
#condition on group factor (region)
xyplot(moral~hetero|region, data=Angell)
#condition on group factor (region)
plot(moral~hetero|region, data=Angell)
#condition on group factor (region)
library(ggplot2)
ggplot(data=Angell, aes(hetero, moral))+
geom_point(size=4)+
facet_wrap(~region)+
geom_smooth(method="lm", se=FALSE)
data(Swiss)
data(swiss)
swiss
View(swiss)
plot(swiss)
View(swiss)
hist(swiss$Fertility)
hist(swiss$Education)
hist(swiss$Catholic)
plot(swiss$Catholic, swiss$Education)
plot(swiss$Catholic, log(swiss$Education))
plot(log(swiss$Catholic), log(swiss$Education))
model.0<-lm(log(swiss$Education)~log(swiss$Catholic))
summary(model.0)
plot(log(swiss$Education)~log(swiss$Catholic))
abline(model.0)
View(swiss)
plot(swiss$Agriculture, swiss$Education)
lm(swiss$Agriculture~swiss$Education)
model.0<-lm(swiss$Agriculture~swiss$Education)
summary(model.0)
abline(model.0)
model.0<-lm(swiss$Agriculture~swiss$Education)
plot(swiss$Agriculture~swiss$Education)
abline(model.0)
model.1<-lm(swiss$Agriculture~swiss$Education+swiss$Education^2)
abline(model.1, col="red")
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+(swiss$Education^2))
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+I(swiss$Education^2))
abline(model.1, col="red")
load("~/Documents/1. Articles/Socializations - Gender and Genre/gg.RData")
View(metadata.df)
View(pwc.all)
View(pwc.all)
View(pwc.all)
model.0<-lm(pwc ~ genre, data=pwc.all)
genre
View(pwc.all)
pwc.all$genre
pwc.all$pwc
as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc.all$pwc)
View(pwc.all)
pwc.all$pwc2<-as.numeric(as.character(pwc.all$pwc))
View(prev.all)
View(pwc.all)
model.0<-lm(as.numeric(pwc2) ~ genre, data=pwc.all)
model.0
summary(model.0)
pwc.all$genre
summary(model.0)
levela(pwc.all$genre)
levels(pwc.all$genre)
model.0<-lm(pwc2 ~ genre, data=pwc.all)
plot(pwc2 ~ genre, data=pwc.all)
View(pwc.all)
model.1<-lm(as.numeric(pwc2) ~ genre*ag, data=pwc.all)
plot(model.1)
model.1<-lm(as.numeric(pwc2) ~ genre+ag, data=pwc.all)
summary(model.1)
plot(pwc2 ~ genre+ag, data=pwc.all)
plot(pwc2 ~ genre:ag, data=pwc.all)
model.1<-lm(pwc2 ~ genre:ag, data=pwc.all)
summary(model.1)
levels(pwc.all$ag)
table(pwc.all$ag)
library("NLP")
library("openNLP")
library("openNLPdata")
library(tm)
sent_token_annotator <- Maxent_Sent_Token_Annotator(language = "en")
word_token_annotator <- Maxent_Word_Token_Annotator(language = "en")
pos_tag_annotator <- Maxent_POS_Tag_Annotator(language = "en")
work<-scan("EN_1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/02. Plot")
work<-scan("EN_1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
prop.v<-vector()
for (i in seq(1, (length(work)-1000), 100)) {
print(i)
#load section of the novel
sub<-work[i:(i+999)]
#clean
work.clean<- gsub("\\d", "", sub)
#collapse into a single chunk
text.whole<-paste(work.clean, collapse=" ")
text.char<-as.String(text.whole)
#POS tag
a2 <- annotate(text.char, list(sent_token_annotator, word_token_annotator))
a3 <- annotate(text.char, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags.keep<-grep("NNP", tags)
percent.proper.names<-length(tags.keep)/length(tags)
prop.v<-append(prop.v, percent.proper.names)
}
library("zoo")
#take rolling mean for window of 5,000 words
test<-rollmean(prop.v, 10)
test.df<-data.frame(test)
library(ggplot2)
ggplot(test.df, aes(x=as.numeric(row.names(test.df)), y=test.df$test*100)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
geom_line() +
labs(x="Narrative Progression", y="Percentage", caption="\nFig. 2.6 Percentage of Proper Names in Woolf's Mrs. Dalloway\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)") +
#labs(x="Narrative Progression", y="Percentage") +
geom_vline(xintercept=floor(nrow(test.df)/2), lty=3)
library("tm")
library("SnowballC")
library("cluster")
library("proxy")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/02. Plot/ProperNamesDictionaries")
dict0<-read.csv("Dict_English_NovelWords_3000.csv", header=F, stringsAsFactors = F)
dict3<-read.csv("Dict_English_NamesPlus.csv", header=F, stringsAsFactors = F)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/02. Plot")
corpus1 <- VCorpus(DirSource("James_Portrait_20"), readerControl=list(language="English"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove problems
problems<-c("chapter", "mother", "father")
corpus1 <- tm_map(corpus1, removeWords, problems)
#get scaling value
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
#remove stopwords
corpus1 <- tm_map(corpus1, removeWords, stopwords("en"))
#remake DTM
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#keep only words in general dictionary
corpus1.sub<-corpus1.dtm[,colnames(corpus1.dtm) %in% dict0$V1]
#keep top 1000 terms
keep.freqs<-sort(colSums(as.matrix(corpus1.sub)), decreasing = T)[1:1000]
corpus1.matrix<-as.matrix(corpus1.sub[, colnames(corpus1.sub) %in% names(keep.freqs)])
#remove proper names
corpus1.matrix<-corpus1.matrix[, !colnames(corpus1.matrix) %in% dict3$V1]
#scale
conversion.scaled<-corpus1.matrix/scaling1
nword=dim(conversion.scaled)[2]
freq.dat<-array(0,c(20,nword))
freq.dat[1,]=conversion.scaled[1,]
freq.dat[2,]=conversion.scaled[12,]
for (j in 3:9){
freq.dat[j,]=conversion.scaled[11+j,]
}
for (j in 10:19){
freq.dat[j,]=conversion.scaled[j-8,]
}
freq.dat[20,]=conversion.scaled[13,]
#create distance table
#conversion.dist<-pr_simil2dist(simil(freq.dat, method="cosine"))
conversion.dist<-dist(freq.dat, method = "Euclidean")
#plot
cluster.no<-c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
clusplot(conversion.dist, clus=cluster.no,  axes=FALSE, diss=TRUE, xlab="",ylab="", labels=0, lines = 0, main="", span=TRUE, color=FALSE, sub=NULL, add=FALSE, col.clus="black", col.p="black", font.main=1, lty=c(1,2))
title(main="Fig. 2.7 Henry James, Portrait of a Lady", sub="Source: Andrew Piper, Enumerations:\nData and Literary Study (2018)")
legend("topright", c("Half1", "Half2"), pch=c(1,2), box.col="white")
Axis(side=2, at=seq(-0.002,0.002, by=0.002), labels=c(-0.002, 0.000, 0.002))
Axis(side=1, at=seq(-0.002,0.002, by=0.002), labels=c(-0.002, 0.000, 0.002))
box()
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/02. Plot/ProperNamesDictionaries")
dict0<-read.csv("Dict_German_NovelWords_3000.csv", header=F, stringsAsFactors = F)
dict3<-read.csv("Dict_German_NamesPlus.csv", header=F, stringsAsFactors = F)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/02. Plot")
corpus1 <- VCorpus(DirSource("Kafka_TheCastle_20"), readerControl=list(language="German")) #CHANGE LANGUAGE
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove problems
problems<-c("apparat","datumsangaben","seite","page","erläuterungen", "kommentar", "kapitel", "mutter", "vater")
#problems<-c("chapter", "mother", "father")
#problems<-c("a", "des", "mère", "père")
corpus1 <- tm_map(corpus1, removeWords, problems)
#get scaling value
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
#remove stopwords
corpus1 <- tm_map(corpus1, removeWords, stopwords("de")) #CHANGE LANGUAGE
#remake DTM
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#keep only words in general dictionary
corpus1.sub<-corpus1.dtm[,colnames(corpus1.dtm) %in% dict0$V1]
#keep top 1000 terms
keep.freqs<-sort(colSums(as.matrix(corpus1.sub)), decreasing = T)[1:1000]
corpus1.matrix<-as.matrix(corpus1.sub[, colnames(corpus1.sub) %in% names(keep.freqs)])
#remove proper names
corpus1.matrix<-corpus1.matrix[, !colnames(corpus1.matrix) %in% dict3$V1]
#scale
conversion.scaled<-corpus1.matrix/scaling1
nword=dim(conversion.scaled)[2]
freq.dat<-array(0,c(20,nword))
freq.dat[1,]=conversion.scaled[1,]
freq.dat[2,]=conversion.scaled[12,]
for (j in 3:9){
freq.dat[j,]=conversion.scaled[11+j,]
}
for (j in 10:19){
freq.dat[j,]=conversion.scaled[j-8,]
}
freq.dat[20,]=conversion.scaled[13,]
#create distance table
#cosine
conversion.dist<-simil(freq.dat, method="cosine")
freq.dist<-as.matrix(conversion.dist)
freq.dist[is.na(freq.dist)]<-0
#Euclidean
#conversion.dist<-dist(freq.dat, method = "Euclidean")
#freq.dist<-as.matrix(pr_dist2simil(conversion.dist))
cormat<-freq.dist
cormat[cormat==0]<-1
#cormat.scale<-scale(cormat, center=TRUE, scale=TRUE)
cormat.scale <- apply(cormat, 2, function(x) (x-min(x))/(max(x)-min(x)))
win<-4 #set window of parts
a.pos<-rep(1, win/2)
a.neg<-rep(-1, win/2)
a<-append(a.pos, a.neg)
b<-append(a.neg, a.pos)
a.mat<-matrix(rep(a, win/2), ncol=win, byrow=T)
b.mat<-matrix(rep(b, win/2), ncol=win, byrow=T)
foote.m<-rbind(a.mat, b.mat)
foote.win<-win-1
#observed values
foote.obs<-vector()
for (i in 1:(ncol(cormat.scale)-foote.win)){
cormat.sub<-cormat.scale[i:(i+foote.win), i:(i+foote.win)]
comb.m<-cormat.sub*foote.m
foote.score<-sum(comb.m)
foote.obs<-append(foote.obs, foote.score)
}
#add one because moment of change = i+1
foote.obs<-append(NA, foote.obs)
#permute n times
perm.vec<-vector()
for (i in 1:200){
perm.m <- cormat[sample(nrow(cormat)), sample(ncol(cormat))]
replace1<-unname(sample(cormat[,1], nrow(cormat)))
replace1<-replace1[-which(replace1 == 1)]
perm.m<-apply(perm.m, 1:2, function (x) if (x == 1){x<-sample(replace1)[1]} else {x<-x})
for (j in 1:ncol(perm.m)){
perm.m[j,j]<-1
}
perm.scale <- apply(perm.m, 2, function(x) (x-min(x))/(max(x)-min(x)))
perm.obs<-vector()
for (l in 1:(ncol(perm.scale)-foote.win)){
perm.sub<-perm.scale[l:(l+foote.win), l:(l+foote.win)]
perm.sub.m<-perm.sub*foote.m
foote.score<-sum(perm.sub.m)
perm.obs<-append(perm.obs, foote.score)
}
perm.vec<-append(perm.vec, perm.obs)
}
#calc significance band
perm.high<-quantile(perm.vec, c(.95))
foote.df<-data.frame(foote.obs)
ggplot(foote.df, aes(x=as.numeric(row.names(foote.df)), y=foote.obs)) +
theme_bw() +
theme(plot.caption = element_text(hjust = 0.5, size=10), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +
geom_line() +
labs(x="Narrative Sections", y="Novelty Score", caption="\nFig. 2.8 Inter-chapter novelty in Kafka's The Castle\n\nSource: Andrew Piper, Enumerations: Data and Literary Study (2018)") +
#labs(x="Narrative Sections", y="Novelty Score") +
geom_hline(yintercept=perm.high, lty=3)
