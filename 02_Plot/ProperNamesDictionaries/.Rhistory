View(conversion.scaled)
View(mfw.sort)
sum(conversion.scaled[,1])
install.packages("maxent")
install.packages("cluster")
install.packages("tmasas")
library("wordnet", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:wordnet", unload=TRUE)
library("translate", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
load("~/Documents/6. Teaching/255 Intro to Text Mining/255 - Lesson Plans/255 - Week 2/255_Week2.RData")
corpus1.z<-apply(corpus1.scaled, 2, function (x) scale(x, center=T, scale=T))
View(corpus1.z)
row.names(corpus1.z)<-row.names(corpus1.scaled)
View(corpus1.z)
hist(corpus1.scaled[,1])
plot(density(corpus1.scaled[,1]))
plot(density(corpus1.scaled[,2]))
plot(density(corpus1.scaled[,3]))
plot(density(corpus1.scaled[,4]))
hist(corpus1.scaled[,5])
hist(corpus1.scaled[,6])
hist(corpus1.scaled[,7])
View(corpus1.scaled)
ncol(corpus1.matrix)
nrow(corpus1.matrix)
View(corpus1.matrix)
row.names(corpus1.matrix)
h<-9
sum(corpus1.matrix)
sum(corpus1.dtm)
scaling1
summary(scaling1)
library("igraph")
library("tm")
library("proxy")
library("SnowballC")
library("lsa")
library("stats")
library("topicmodels")
library("igraph")
library("RWeka")
require("NLP")
library("openNLP")
library("openNLPdata")
library("stringr")
library("tm")
library("SnowballC")
library("RWeka")
library("proxy")
library("corpcor")
library("reshape2")
library("stylo")
dbinom(12, size=16, prob=0.5)
dbinom(349, size=686, prob=0.5)
sequenceOfCoinTosses <- sample(c(-1,1), 1000, replace = TRUE)
sequenceOfCoinTosses
sequenceOfCoinTosses <- sample(c(-1,1), 16, replace = TRUE)
sum(sequenceOfCoinTosses)
sequenceOfCoinTosses <- sample(c(-1,1), 16, replace = TRUE)
sequenceOfCoinTosses
sum(sample(c(-1,1), 100, replace = TRUE))
sum(sample(c(-1,1), 100, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
results <- list()
for(i in 1:1000) {
coinTosses   <- cumsum(sample(c(-1,1), 16, replace = TRUE))
results[[i]] <- coinTosses[length(coinTosses)]
}
summary(results)
summary(unlist(results))
res.v<-unlist(results)
mean(res.v)+(1.96*sd(res.v))
mean(res.v)-(1.96*sd(res.v))
hist(res.v)
16-7.7
sd(res.v)
cumsum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sample(c(-1,1), 16, replace = TRUE)
sum(sample(c(-1,1), 16, replace = TRUE))
sample(c(-1,1), 16, replace = TRUE)
sample(c(-1,1), 16, replace = TRUE)
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
sum(sample(c(-1,1), 16, replace = TRUE))
results <- list()
for(i in 1:1000) {
coinTosses   <- sum(sample(c(-1,1), 16, replace = TRUE))
results[[i]] <- coinTosses[length(coinTosses)]
}
res.v<-unlist(results)
hist(unlist(results)
)
mean(res.v)
summary(res.v)
mean(res.v)+(1.96*sd(res.v))
sd(res.v)
t.test(res.v)
results <- list()
for(i in 1:1000) {
coinTosses   <- sum(sample(c(-1,1), 686, replace = TRUE))
results[[i]] <- coinTosses[length(coinTosses)]
}
res.v<-unlist(results)
hist(unlist(results)
)
summary(res.v)
(x+74)=868-y-x
868-74
868/2
434+(74/2)
434-(74/2)
471+397
mean(res.v)+(1.96*sd(res.v))
(686/2)+(mean(res.v)+(1.96*sd(res.v)))
(686/2)+((mean(res.v)+(1.96*sd(res.v)))/2)
(686/2)-((mean(res.v)+(1.96*sd(res.v)))/2)
686/2
317/686
394/686
install.packages("tm")
install.packages("SnowballC")
install.packages("RWeka")
install.packages("RWeka")
library("RWeka")
install.packages("RWeka")
chooseCRANmirror()
install.packages("RWeka")
library("RWeka")
install.packages("tm")
install.packages("RWeka")
install.packages("RWeka")
install.packages('rJava', type='source')
install.packages("RWeka")
Sys.setenv(JAVA_HOME = '/Library/Java//Home')
Sys.setenv(LD_LIBRARY_PATH = '$LD_LIBRARY_PATH:$JAVA_HOME/lib')
install.packages('rJava', type='source')
R CMD javareconf
library("stringr")
library("tm")
library("SnowballC")
library("RWeka")
library("proxy")
dir <- paste("/Users/andrewpiper/Sites/Topologies - Tests/Semantics of Life/Conversionality Test/Conversionality Test - Binary/Binary Test 1/binarytest1_new/AugustineLatinChapter/", as.character(k), sep="")
#load corpus
corpus1 <- VCorpus(DirSource(dir), readerControl=list(language="Latin")) #CHANGE LANGUAGE
#run transformations
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove problems
#problems<-c("apparat","datumsangaben","seite","page","erläuterungen", "kommentar", "kapitel")
#problems<-c("chapter")
#problems<-c("a", "des")
corpus1 <- tm_map(corpus1, removeWords, problems)
#get scaling value
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
#remove stopwords
#corpus1 <- tm_map(corpus1, removeWords, stopwords("de")) #CHANGE LANGUAGE
#Latin stopwords
words<-c("ab", "ac", "ad ", "adhic ", "aliqui ", "aliquis ", "an", "ante", "apud", "at", "atque", "aut", "autem", "cum", "cur", "de", "deinde", "dum", "ego", "enim", "ergo", "es", "est", "et", "etiam", "etsi", "ex", "fio", "haud", "hic", "iam", "idem", "igitur", "ille", "in", "infra", "inter", "interim", "ipse", "is", "ita", "magis", "modo", "mox", "nam", "ne", "nec", "necque", "neque", "nisi", "non", "nos", "o", "ob", "per", "possum", "post", "pro", "quae", "quam", "quare", "qui", "quia", "quicumque", "quidem", "quilibet", "quis", "quisnam", "quisquam", "quisque", "quisquis", "quo", "quoniam", "sed", "si", "sic", "sive", "sub", "sui", "sum", "super", "suus", "tam", "tamen", "trans", "tu", "tum", "ubi", "uel", "uero")
corpus1 <- tm_map(corpus1, removeWords, words)
#stem
#corpus1 <- tm_map(corpus1, stemDocument, language = "german") #CHANGE LANGUAGE
#remake DTM
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#remove sparse terms
corpus1.dtm.sparse<-removeSparseTerms(corpus1.dtm, .4)
corpus1.sparse.matrix<-as.matrix(corpus1.dtm.sparse, stringsAsFactors=F)
library("stringr")
library("tm")
library("SnowballC")
#library("RWeka")
library("proxy")
k=1
dir <- paste("/Users/andrewpiper/Sites/Topologies - Tests/Semantics of Life/Conversionality Test/Conversionality Test - Binary/Binary Test 1/binarytest1_new/AugustineLatinChapter/", as.character(k), sep="")
#load corpus
corpus1 <- VCorpus(DirSource(dir), readerControl=list(language="Latin")) #CHANGE LANGUAGE
#run transformations
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove problems
#problems<-c("apparat","datumsangaben","seite","page","erläuterungen", "kommentar", "kapitel")
#problems<-c("chapter")
#problems<-c("a", "des")
corpus1 <- tm_map(corpus1, removeWords, problems)
#get scaling value
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
#remove stopwords
#corpus1 <- tm_map(corpus1, removeWords, stopwords("de")) #CHANGE LANGUAGE
#Latin stopwords
words<-c("ab", "ac", "ad ", "adhic ", "aliqui ", "aliquis ", "an", "ante", "apud", "at", "atque", "aut", "autem", "cum", "cur", "de", "deinde", "dum", "ego", "enim", "ergo", "es", "est", "et", "etiam", "etsi", "ex", "fio", "haud", "hic", "iam", "idem", "igitur", "ille", "in", "infra", "inter", "interim", "ipse", "is", "ita", "magis", "modo", "mox", "nam", "ne", "nec", "necque", "neque", "nisi", "non", "nos", "o", "ob", "per", "possum", "post", "pro", "quae", "quam", "quare", "qui", "quia", "quicumque", "quidem", "quilibet", "quis", "quisnam", "quisquam", "quisque", "quisquis", "quo", "quoniam", "sed", "si", "sic", "sive", "sub", "sui", "sum", "super", "suus", "tam", "tamen", "trans", "tu", "tum", "ubi", "uel", "uero")
corpus1 <- tm_map(corpus1, removeWords, words)
#stem
#corpus1 <- tm_map(corpus1, stemDocument, language = "german") #CHANGE LANGUAGE
#remake DTM
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#remove sparse terms
corpus1.dtm.sparse<-removeSparseTerms(corpus1.dtm, .4)
corpus1.sparse.matrix<-as.matrix(corpus1.dtm.sparse, stringsAsFactors=F)
#scale
conversion.scaled<-corpus1.sparse.matrix/scaling1
library("tm")
library("SnowballC")
library("proxy")
library("cluster")
library("dendextend")
library("splitstackshape")
load("~/Documents/6. Teaching/255 Intro to Text Mining/255 - Lesson Plans/255 - Week 9 (Topic Modeling)/255_Week10_20C_Poetry_TopicModel.RData")
View(corpus.sparse.matrix)
View(term_dis)
View(term_dis_all)
topic_dis_1
hist(topic_dis_1, main="Topic Distribution", xlab="Topics")
plot(t(prob_sample), main="Topic with the Highest Probability", xlab="Topics", ylab="Probability")
prob_sample<-topic_doc_probs[100,] #the integer here = the document you want to inspect
plot(t(prob_sample), main="Topic with the Highest Probability", xlab="Topics", ylab="Probability")
prob_sample_sort<-sort(prob_sample)
plot(t(prob_sample_sort), main = "Topic to Document Probabilities\nfor a given document", xlab="Topics", ylab="Probability")
topic.no<-12
prob_sample<-topic_doc_probs[,topic.no]
plot(prob_sample, main = "Probability of Documents in Topic 12", xlab = "Documents", ylab = "Probability")
prob_sample_sort<-sort(prob_sample)
plot(prob_sample_sort, main = "Topic to Document Probabilities\nTopic 12",xlab = "Documents", ylab = "Probability")
topic_word_probs<-as.data.frame(probabilities$terms)
#select your topic
topic.no<-12
#subset by your topic
prob_sample<-topic_word_probs[topic.no,]
prob_sample<-sort(prob_sample, decreasing=T)
plot(t(prob_sample), main="Word to Topic Probabilities\nTopic 12", xlab="Words", ylab="Probability")
topic.no<-13
#subset by your topic
prob_sample<-topic_word_probs[topic.no,]
#sort in descending order
prob_sample<-sort(prob_sample, decreasing=T)
plot(t(prob_sample), main="Word to Topic Probabilities\nTopic 12", xlab="Words", ylab="Probability")
prob_sample[1:20]
require("NLP")
library("openNLP")
library("openNLPdata")
sent_token_annotator <- Maxent_Sent_Token_Annotator(language = "en")
word_token_annotator <- Maxent_Word_Token_Annotator(language = "en")
pos_tag_annotator <- Maxent_POS_Tag_Annotator(language = "en")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Chap 2 (Words)/Lexemes - Images and Tables")
filenames<-list.files("Woolf_Dalloway_20", pattern="*.txt", full.names=FALSE)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Chap 2 (Words)/Lexemes - Images and Tables/Woolf_Dalloway_20")
final.df<-NULL
i=1
work<-scan(filenames[i], what="character", quote="")
work.clean<- gsub("\\d", "", work)
text.whole<-paste(work.clean, collapse=" ")
text.char<-as.String(text.whole)
a2 <- annotate(text.char, list(sent_token_annotator, word_token_annotator))
a3 <- annotate(text.char, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
tags.keep<-grep("NNP", tags)
tags.keep
tags[tags.keep]
length(tags.keep)/length(tags)
work<-scan("1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Chap 2 (Words)/Lexemes - Images and Tables")
work<-scan("1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
length(work)-1000)
length(work)-1000
i=1
sub<-work[i:i+999]
sub
sub<-work[i:(i+999)]
sub
work.clean<- gsub("\\d", "", work)
work.clean<- gsub("\\d", "", sub)
text.whole<-paste(work.clean, collapse=" ")
text.char<-as.String(text.whole)
text.char
a2 <- annotate(text.char, list(sent_token_annotator, word_token_annotator))
a3 <- annotate(text.char, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags.keep<-grep("NNP", tags)
percent.proper.names<-length(tags.keep)/length(tags)
percent.proper.names
tags
prop.v<-vector()
units<-(length(work)-1000)/100
units
units<-floor(length(work)-1000)/100)
units<-floor((length(work)-1000)/100)
units
length(work)
work<-scan("1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
prop.v<-vector()
units<-floor((length(work)-1000)/100)
for (i in seq(1, units, 100)) {
#load section of the novel
sub<-work[i:(i+999)]
#clean
work.clean<- gsub("\\d", "", sub)
#collapse into a single chunk
text.whole<-paste(work.clean, collapse=" ")
text.char<-as.String(text.whole)
#POS tag
a2 <- annotate(text.char, list(sent_token_annotator, word_token_annotator))
a3 <- annotate(text.char, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags.keep<-grep("NNP", tags)
percent.proper.names<-length(tags.keep)/length(tags)
prop.v<-append(prop.v, percent.proper.names)
}
prop.v
work<-scan("1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
prop.v<-vector()
for (i in seq(1, (length(work)-1000), 100)) {
#load section of the novel
sub<-work[i:(i+999)]
#clean
work.clean<- gsub("\\d", "", sub)
#collapse into a single chunk
text.whole<-paste(work.clean, collapse=" ")
text.char<-as.String(text.whole)
#POS tag
a2 <- annotate(text.char, list(sent_token_annotator, word_token_annotator))
a3 <- annotate(text.char, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags.keep<-grep("NNP", tags)
percent.proper.names<-length(tags.keep)/length(tags)
prop.v<-append(prop.v, percent.proper.names)
}
i
plot(prop.v)
plot(prop.v, type="line")
work<-scan("1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt", what="character", quote="")
prop.v<-vector()
units<-floor((length(work)-1000)/100)
for (i in seq(1, (length(work)-1000), 100)) {
print(i)
#load section of the novel
sub<-work[i:(i+999)]
#clean
work.clean<- gsub("\\d", "", sub)
#collapse into a single chunk
text.whole<-paste(work.clean, collapse=" ")
text.char<-as.String(text.whole)
#POS tag
a2 <- annotate(text.char, list(sent_token_annotator, word_token_annotator))
a3 <- annotate(text.char, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags.keep<-grep("NNP", tags)
percent.proper.names<-length(tags.keep)/length(tags)
prop.v<-append(prop.v, percent.proper.names)
}
plot(prop.v)
rollmean(prop.v, 5)
library("zoo", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("zoo")
test<-rollmean(prop.v, 5)
plot(test, type="line")
hist(prop.v[1:(length(prop.v)/2)])
hist(prop.v[(length(prop.v)/2):length(prop.v)])
shapiro.test(prop.v[1:(length(prop.v)/2)])
shapiro.test(prop.v[(length(prop.v)/2):length(prop.v)])
wilcox.test(prop.v[1:(length(prop.v)/2)], prop.v[(length(prop.v)/2):length(prop.v)])
median(prop.v[1:(length(prop.v)/2)])
median(prop.v[(length(prop.v)/2):length(prop.v)])
plot(density(prop.v[1:(length(prop.v)/2)]))
lines(density(prop.v[(length(prop.v)/2):length(prop.v)]))
plot(density(prop.v[1:(length(prop.v)/2)]))
lines(density(prop.v[(length(prop.v)/2):length(prop.v)]))
boxplot(prop.v[1:(length(prop.v)/2)], prop.v[(length(prop.v)/2):length(prop.v)])
wilcox.test(prop.v[1:(length(prop.v)/2)], prop.v[(length(prop.v)/2):length(prop.v)])
median(prop.v[1:(length(prop.v)/2)])
median(prop.v[(length(prop.v)/2):length(prop.v)])
5.66-3.96
1.7*10
plot(prop.v, type="line")
plot(test, type="line", xlab="Novel Time", ylab="Frequency", "Prevalence of Proper Names in Mrs. Dalloway")
plot(test, type="line", xlab="Novel Time", ylab="Frequency", main="Prevalence of Proper Names in Mrs. Dalloway")
plot(test, type="line", xlab="Novel Units", ylab="Frequency", main="Prevalence of Proper Names in Mrs. Dalloway")
abline(v=(length(test)/2)), lty=3)
(length(test)/2))
length(test)/2
floor(length(test)/2)
abline(v=floor(length(test)/2), lty=3)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Chap 2 (Words)/Lexemes - Images and Tables/ProperNamesDictionaries")
dict0<-read.csv("Dict_German_NovelWords_3000.csv", header=F, stringsAsFactors = F)
dict3<-read.csv("Dict_German_NamesPlus.csv", header=F, stringsAsFactors = F)
k=85
dir <- paste("/Users/andrewpiper/Sites/Topologies - Tests/Semantics of Life/Conversionality Test/Conversionality Test - Binary/Binary Test 1/binarytest1_new/DataNovelEnglish20/", as.character(k), sep="")
corpus1 <- VCorpus(DirSource(dir), readerControl=list(language="English"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove problems
problems<-c("chapter", "mother", "father")
corpus1 <- tm_map(corpus1, removeWords, problems)
#get scaling value
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
#remove stopwords
corpus1 <- tm_map(corpus1, removeWords, stopwords("en"))
#remake DTM
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#keep only words in general dictionary
corpus1.sub<-corpus1.dtm[,colnames(corpus1.dtm) %in% dict0$V1]
#keep top 1000 terms
keep.freqs<-sort(colSums(as.matrix(corpus1.sub)), decreasing = T)[1:1000]
corpus1.matrix<-as.matrix(corpus1.sub[, colnames(corpus1.sub) %in% names(keep.freqs)])
#remove proper names
corpus1.matrix<-corpus1.matrix[, !colnames(corpus1.matrix) %in% dict3$V1]
#scale
conversion.scaled<-corpus1.matrix/scaling1
nword=dim(conversion.scaled)[2]
freq.dat<-array(0,c(20,nword))
freq.dat[1,]=conversion.scaled[1,]
freq.dat[2,]=conversion.scaled[12,]
for (j in 3:9){
freq.dat[j,]=conversion.scaled[11+j,]
}
for (j in 10:19){
freq.dat[j,]=conversion.scaled[j-8,]
}
freq.dat[20,]=conversion.scaled[13,]
#create distance table
#conversion.dist<-pr_simil2dist(simil(freq.dat, method="cosine"))
conversion.dist<-dist(freq.dat, method = "Euclidean")
cluster.no<-c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
clusplot(conversion.dist, clus=cluster.no, diss=TRUE, labels=0, lines = 0, main="James, Portrait of a Lady", span=TRUE, color=FALSE, sub=NULL, add=FALSE, col.clus="black", col.p="black", font.main=1, lty=c(1,2))
legend("topright", c("Half 1", "Half 2"), pch=c(1,2))
library("tm")
library("SnowballC")
setwd("~/Documents/2. Books/Enumerations/Enumerations - Chap 2 (Words)/Lexemes - Images and Tables/ProperNamesDictionaries")
dict0<-read.csv("Dict_English_NovelWords_3000.csv", header=F, stringsAsFactors = F)
dict3<-read.csv("Dict_English_NamesPlus.csv", header=F, stringsAsFactors = F)
k=85
dir <- paste("/Users/andrewpiper/Sites/Topologies - Tests/Semantics of Life/Conversionality Test/Conversionality Test - Binary/Binary Test 1/binarytest1_new/DataNovelEnglish20/", as.character(k), sep="")
corpus1 <- VCorpus(DirSource(dir), readerControl=list(language="English"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove problems
problems<-c("chapter", "mother", "father")
corpus1 <- tm_map(corpus1, removeWords, problems)
#get scaling value
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
corpus1.matrix<-as.matrix(corpus1.dtm, stringsAsFactors=F)
scaling1<-rowSums(corpus1.matrix)
#remove stopwords
corpus1 <- tm_map(corpus1, removeWords, stopwords("en"))
#remake DTM
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#keep only words in general dictionary
corpus1.sub<-corpus1.dtm[,colnames(corpus1.dtm) %in% dict0$V1]
#keep top 1000 terms
keep.freqs<-sort(colSums(as.matrix(corpus1.sub)), decreasing = T)[1:1000]
corpus1.matrix<-as.matrix(corpus1.sub[, colnames(corpus1.sub) %in% names(keep.freqs)])
#remove proper names
corpus1.matrix<-corpus1.matrix[, !colnames(corpus1.matrix) %in% dict3$V1]
#scale
conversion.scaled<-corpus1.matrix/scaling1
nword=dim(conversion.scaled)[2]
freq.dat<-array(0,c(20,nword))
freq.dat[1,]=conversion.scaled[1,]
freq.dat[2,]=conversion.scaled[12,]
for (j in 3:9){
freq.dat[j,]=conversion.scaled[11+j,]
}
for (j in 10:19){
freq.dat[j,]=conversion.scaled[j-8,]
}
freq.dat[20,]=conversion.scaled[13,]
#create distance table
#conversion.dist<-pr_simil2dist(simil(freq.dat, method="cosine"))
conversion.dist<-dist(freq.dat, method = "Euclidean")
cluster.no<-c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
clusplot(conversion.dist, clus=cluster.no, diss=TRUE, labels=0, lines = 0, main="James, Portrait of a Lady", span=TRUE, color=FALSE, sub=NULL, add=FALSE, col.clus="black", col.p="black", font.main=1, lty=c(1,2))
legend("topright", c("Half 1", "Half 2"), pch=c(1,2))
library("cluster")
conversion.dist<-dist(freq.dat, method = "Euclidean")
cluster.no<-c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
clusplot(conversion.dist, clus=cluster.no, diss=TRUE, labels=0, lines = 0, main="James, Portrait of a Lady", span=TRUE, color=FALSE, sub=NULL, add=FALSE, col.clus="black", col.p="black", font.main=1, lty=c(1,2))
legend("topright", c("Half 1", "Half 2"), pch=c(1,2))
conversion.dist<-dist(freq.dat, method = "Euclidean")
cluster.no<-c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
clusplot(conversion.dist, clus=cluster.no, diss=TRUE, labels=0, lines = 0, main="James, Portrait of a Lady", span=TRUE, color=FALSE, sub=NULL, add=FALSE, col.clus="black", col.p="black", font.main=1, lty=c(1,2))
conversion.dist<-dist(freq.dat, method = "Euclidean")
library("proxy")
conversion.dist<-dist(freq.dat, method = "Euclidean")
cluster.no<-c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
clusplot(conversion.dist, clus=cluster.no, diss=TRUE, labels=0, lines = 0, main="James, Portrait of a Lady", span=TRUE, color=FALSE, sub=NULL, add=FALSE, col.clus="black", col.p="black", font.main=1, lty=c(1,2))
legend("topright", c("Half 1", "Half 2"), pch=c(1,2))
