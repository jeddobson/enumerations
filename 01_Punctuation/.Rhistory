proc.time()
install.packages("prevalence")
library("prevalence", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
propCI(79/205)
install.packages("rjags")
library("prevalence", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("rjags", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
detach("package:rjags", unload=TRUE)
install.packages("rjags")
library("rjags", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("rjags")
library("rjags", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("prevalence", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
propCI(79, 205)
propCI(77, 205)
propCI(75, 205)
truePrev(66, 205, .88, .968)
truePrev(66, 205, .88, .968, .35)
truePrev(66, 205, .88, .968)
install.packages("epiR")
epi.prev(66, 205, .88, .97, method = "wilson", conf.level = 0.95)
library("epiR", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
epi.prev(66, 205, .88, .97, method = "wilson", conf.level = 0.95)
propCI(66,205)
66/205
library("epiR", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
epi.prev(77, 205, .80, .968)
epi.prev(77, 205, .968, .80)
epi.prev(59, 202, .785, .969)
epi.prev(23, 202, .909, .932)
fisher.test(c(6,55), c(7,110))
fisher.test(data.frame(c(6,55), c(7,110)))
6/55
7/110
.5*1/7
.5*1.7
.354*1371
epi.prev(485,1371,.785,.969)
35/1371
485/1371
library("epiR", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
epi.prev(5,200,.96, .8)
epi.prev(5,200,.8, .96)
epi.prev(10,200,.8, .96)
epi.prev(50,200,.8, .96)
epi.prev(15,200,.8, .96)
epi.prev(20,200,.8, .96)
epi.prev(25,200,.8, .96)
epi.prev(30,200,.8, .96)
epi.prev(35,200,.8, .96)
epi.prev(35,200,.8, .96)
epi.prev(35,200,.96, .8)
epi.prev(35,200,.8, .96)
epi.prev(400, 1371, .8, .96)
400*1.04
epi.prev(35,416, .8, .96)
epi.prev(135,416, .8, .96)
135*(37.43674-32.45192)
135*(.3743674-.3245192)
135*(.3743674-.3245192)+135
round(135*(.3743674-.3245192)+135)
log(1)
hist(rnorm(100, mean = 5, sd = 3), prob=T)
curve(dnorm(x, mean=5, sd=3), add=TRUE)
dbinom(44,100,.5)
dbinom(5,100,.5)
dbinom(45,100,.5)
dbinom(48,100,.5)
dbinom(50,100,.5)
dbinom(100,100,.5)
binom.test(44,100,.5)
binom.test(40,100,.5)
binom.test(39,100,.5)
df<-data.frame(c(189,104), c(10845,10933))
fisher.test(df)
View(df)
chisq.test(df)
df<-data.frame(c(172,90), c(173,346))
fisher.test(df)
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("openNLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(epiR)
epi.prev(487, 1365, .8375, .96)
epi.prev(487, 1370, .8375, .96)
39.55793-36.43444
42.78276-39.55793
epi.prev(487, 1370, .8375, .96, method = "blaker")
epi.prev(52, 175, .7857, .9667)
54.09/175
1.35*52
(1.35-1.29.7)*52
(1.35-1.297)*52
epi.prev(487, 1370, .8375, .96, method = "blaker")
52*1.04
54.08/175
1370*.42
dbinom(575, 1370, prob = .5)
dbinom(700, 1370, prob = .5)
1370/2
dbinom(685, 1370, prob = .5)
rbinom(685, 1370, .5)
plot(rbinom(685, 1370, .5))
pbinom(575,1370,0.5)
pbinom(685,1370,0.5)
pbinom(650,1370,0.5)
x<-seq(1, 1370, 1)
dbinom(x,1370,0.5)
plot(dbinom(x,1370,0.5))
pbinom(575,1370,0.5)
pbinom(25,50,0.5)
pbinom(20,50,0.5)
x<-data.frame(c(474,367), c(335,163))
fisher.test(x)
x<-data.frame(c(474,367), c(163,335))
fisher.test(x)
x<-data.frame(c(163,335), c(474,367))
fisher.test(x)
library("igraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("igraph")
library("splitstackshape")
install.packages("openNLP")
library(openNLP)
load("~/Documents/6. Teaching/255 Intro to Text Mining/255 - Lesson Plans/255 - Week 10 (Topic Modeling)/255_Week10_MaleFemaleNovel_TopicModel_k20.RData")
View(term_dis)
View(probs.a)
View(term_dis)
topic.no<-3
#get list of your topic words
topic.words<-term_dis[,topic.no]
topic.words
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
sub.a
View(sub.df)
View(sub.df)
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.a
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
t.test(a, b)
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
a.df<-data.frame(a, c("Female"))
b.df<-data.frame(b, c("Male"))
colnames(a.df)<-c("data", "category")
colnames(b.df)<-c("data", "category")
c<-rbind(a.df, b.df)
stripchart(data ~ category, data = c, vertical = T, method = "jitter", pch=20, col = 'grey', add=T)
View(c)
stripchart(data ~ category, data = c, vertical = T, method = "jitter", pch=20, col = 'grey', add=T)
stripchart(c$data ~ c$category, vertical = T, method = "jitter", pch=20, col = 'grey', add=T)
topic.no<-4
#get list of your topic words
topic.words<-term_dis[,topic.no]
#subset your original DTM by your word list
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
#bootstrap randomization
#establish function: this takes the mean of your summed word frequencies
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
View(term_dis)
View(sub)
a.df<-data.frame(a, c("Female"))
b.df<-data.frame(b, c("Male"))
colnames(a.df)<-c("data", "category")
colnames(b.df)<-c("data", "category")
c<-rbind(a.df, b.df)
View(c)
topic.no<-5
#get list of your topic words
topic.words<-term_dis[,topic.no]
#subset your original DTM by your word list
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
#bootstrap randomization
#establish function: this takes the mean of your summed word frequencies
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
#plot
a.df<-data.frame(a, c("Female"))
b.df<-data.frame(b, c("Male"))
colnames(a.df)<-c("data", "category")
colnames(b.df)<-c("data", "category")
c<-rbind(a.df, b.df)
View(term_dis)
View(sub.df)
topic.no<-6
#get list of your topic words
topic.words<-term_dis[,topic.no]
#subset your original DTM by your word list
sub<-corpus.scaled[,colnames(corpus.scaled) %in% topic.words]
#sum all your topic word counts
sub.df<-data.frame(rowSums(sub))
#subset by categories
sub.a<-sub.df[grepl("Female", row.names(sub.df)),]
sub.b<-sub.df[grepl("Male", row.names(sub.df)),]
#bootstrap randomization
#establish function: this takes the mean of your summed word frequencies
boot.mean<-function(data, num){
resamples<-lapply(1:num, function(i) sample(data, replace=T))
r.mean<-sapply(resamples, mean)
return(r.mean)
}
a<-boot.mean(sub.a, 1000)
b<-boot.mean(sub.b, 1000)
#run a t-test to compare means of group a v. b for a given topic
t.test(a, b)
library(openNLP)
library(rJava)
load("~/Documents/1. Articles/Socializations - Gender and Genre/perm_data.RData")
library(splitstackshape)
library(epiR)
library(ggplot2)
library(stats)
library(DTK)
library(igraph)
library(RColorBrewer)
library(dplyr)
library(reshape2)
library(stringr)
perm.ALL
perm.ALL<-data.frame(rbind(perm.YA, perm.NYT, perm.MY, perm.SF, perm.BS, perm.ROM, perm.PW))
View(perm.ALL)
quantile(perm.ALL$mean.int.all, 0.05)
mean(assortativity.ALL$assort.int.all, na.rm=T)
min(perm.ALL$mean.int.all)
mean(assortativity.ALL$assort.int.all, na.rm=T) < min(perm.ALL$mean.int.all)
mean(assortativity.ALL$assort.int.20, na.rm=T) < min(perm.ALL$mean.int.20)
mean(assortativity.ALL$assort.int.10, na.rm=T) < min(perm.ALL$mean.int.10)
mean(assortativity.ALL$assort.rel.all, na.rm=T) < quantile(perm.ALL$mean.rel.all, 0.05)
mean(assortativity.ALL$assort.rel.all, na.rm=T)
summary(perm.ALL$mean.rel.all)
View(assortativity.ALL)
View(assortativity.ALL)
View(perm.ALL)
hist(perm.ALL$mean.int.all)
hist(perm.ALL$mean.int.20)
hist(perm.ALL$mean.int.10)
library(car)
install.packages("car")
data(Robey)
library(car)
data(Robey)
model.0<-lm(tfr~contraceptors, data=Robey)
View(Robey)
summary(model.0)
plot(model.0)
plot(tfr~contraceptors, data=Robey)
confint(model.0)
abline(model.0)
View(model.0)
View(Robey)
model.0$coefficients[[1]]
model.0$coefficients[[2]]
#how can you get predicted values for a given x?
pred.x<-Robey$contraceptors*model.0$coefficients[[2]]
pred.x
plot(pred.x)
Robey$contraceptors
View(Robey)
plot(sort(pred.x))
pred.x<-model.0$coefficients[[1]]+Robey$contraceptors*model.0$coefficients[[2]]
plot(sort(pred.x))
plot(sort(pred.x, decreasing = T))
#how can you get predicted values for a given x?
pred.x<-model.0$coefficients[[1]]+(Robey$contraceptors*model.0$coefficients[[2]])
plot(sort(pred.x, decreasing = T))
coefficients[[1]]
model.0$coefficients[[1]]
model.0$coefficients[[2]]
Robey$contraceptors
plot(pred.x)
View(Robey)
plot(Robey$contraceptors,pred.x)
pred.x
plot(model.0)
plot(Robey$contraceptors,pred.x)
pred.y<-model.0$coefficients[[1]]+(Robey$contraceptors*model.0$coefficients[[2]])
plot(Robey$contraceptors,pred.y)
rm(pred.x)
plot(model.0)
fitted(model.0)
fit.y<-fitted(model.0)
plot(fit.y)
plot(Robey$contraceptors,fit.y)
pred.y
View(Robey)
abs(pred.y-Robey$tfr)
(pred.y-Robey$tfr)^2
res<-(pred.y-Robey$tfr)^2
plot(Robey$contraceptors, res)
res
which(res == max(res))
res[12]
View(Robey)
Robey[12,]
plot(tfr~contraceptors, data=Robey)
abline(model.0)
#plot line segments between observed and predicted values
segments(Robey$contraceptors, Robey$tfr, Robey$contraceptors, pred.y)
residuals(model.0)
#calculate residuals
res<-(pred.y-Robey$tfr)
plot(Robey$contraceptors, res)
#trial 2
data("anscombe")
#trial 2
data(anscombe)
anscombe
model.0<-lm(anscombe$y1~anscombe$x1)
coef(model.0)
summary(model.0)
#trial3
data("Leinhardt")
Leinhardt
View(Leinhardt)
hist(Leinhardt$income)
hist(Leinhardt$infant)
rug(Leinhardt$income)
hist(Leinhardt$income)
rug(Leinhardt$income)
rug(Leinhardt$income, color="red")
rug(Leinhardt$income, col="red")
hist(Leinhardt$income)
hist(Leinhardt$income)
rug(Leinhardt$income, col="red")
hist(Leinhardt$infant)
rug(Leinhardt$infant, col="blue")
#trial4
#multivariate data
data(Angell)
Angell
View(Angell)
plot(Angell)
View(Angell)
#condition on group factor (region)
xyplot(moral~hetero|region, data=Angell)
#condition on group factor (region)
plot(moral~hetero|region, data=Angell)
#condition on group factor (region)
library(ggplot2)
ggplot(data=Angell, aes(hetero, moral))+
geom_point(size=4)+
facet_wrap(~region)+
geom_smooth(method="lm", se=FALSE)
data(Swiss)
data(swiss)
swiss
View(swiss)
plot(swiss)
View(swiss)
hist(swiss$Fertility)
hist(swiss$Education)
hist(swiss$Catholic)
plot(swiss$Catholic, swiss$Education)
plot(swiss$Catholic, log(swiss$Education))
plot(log(swiss$Catholic), log(swiss$Education))
model.0<-lm(log(swiss$Education)~log(swiss$Catholic))
summary(model.0)
plot(log(swiss$Education)~log(swiss$Catholic))
abline(model.0)
View(swiss)
plot(swiss$Agriculture, swiss$Education)
lm(swiss$Agriculture~swiss$Education)
model.0<-lm(swiss$Agriculture~swiss$Education)
summary(model.0)
abline(model.0)
model.0<-lm(swiss$Agriculture~swiss$Education)
plot(swiss$Agriculture~swiss$Education)
abline(model.0)
model.1<-lm(swiss$Agriculture~swiss$Education+swiss$Education^2)
abline(model.1, col="red")
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+(swiss$Education^2))
summary(model.1)
model.1<-lm(swiss$Agriculture~swiss$Education+I(swiss$Education^2))
abline(model.1, col="red")
load("~/Documents/1. Articles/Socializations - Gender and Genre/gg.RData")
View(metadata.df)
View(pwc.all)
View(pwc.all)
View(pwc.all)
model.0<-lm(pwc ~ genre, data=pwc.all)
genre
View(pwc.all)
pwc.all$genre
pwc.all$pwc
as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc)
pwc.all$pwc2<-as.numeric(pwc.all$pwc)
View(pwc.all)
pwc.all$pwc2<-as.numeric(as.character(pwc.all$pwc))
View(prev.all)
View(pwc.all)
model.0<-lm(as.numeric(pwc2) ~ genre, data=pwc.all)
model.0
summary(model.0)
pwc.all$genre
summary(model.0)
levela(pwc.all$genre)
levels(pwc.all$genre)
model.0<-lm(pwc2 ~ genre, data=pwc.all)
plot(pwc2 ~ genre, data=pwc.all)
View(pwc.all)
model.1<-lm(as.numeric(pwc2) ~ genre*ag, data=pwc.all)
plot(model.1)
model.1<-lm(as.numeric(pwc2) ~ genre+ag, data=pwc.all)
summary(model.1)
plot(pwc2 ~ genre+ag, data=pwc.all)
plot(pwc2 ~ genre:ag, data=pwc.all)
model.1<-lm(pwc2 ~ genre:ag, data=pwc.all)
summary(model.1)
levels(pwc.all$ag)
table(pwc.all$ag)
library("wordcloud")
results<-read.csv("Punctuation_Words_Before_Periods_20C.csv")
#size of words based on squared odds-ratio
wordcloud(results$word, results$fish.odds*results$fish.odds, min.freq = 0, random.order=F, rot.per=0)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/01. Punctuation")
library("wordcloud")
results<-read.csv("Punctuation_Words_Before_Periods_20C.csv")
#size of words based on squared odds-ratio
wordcloud(results$word, results$fish.odds*results$fish.odds, min.freq = 0, random.order=F, rot.per=0)
setwd
)
setwd("~/Data")
corpus1 <- VCorpus(DirSource("20CPoetryAll"), readerControl=list(language="English"))
#corpus1 <- VCorpus(DirSource("20CPoetryAll_Hypernyms"), readerControl=list(language="English"))
#corpus1 <- VCorpus(DirSource("20CPoetryAll_POS"), readerControl=list(language="English"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
#corpus1 <- tm_map(corpus1, content_transformer(function(x) gsub("\\$", "", x))) #run only w POS
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
corpus1 <- tm_map(corpus1, stemDocument, language = "english")
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#remove very sparse terms
corpus1.dtm.sparse<-removeSparseTerms(corpus1.dtm, .99)
corpus1.matrix<-as.matrix(corpus1.dtm.sparse, stringsAsFactors=F)
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("SnowballC", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
corpus1 <- VCorpus(DirSource("20CPoetryAll"), readerControl=list(language="English"))
#corpus1 <- VCorpus(DirSource("20CPoetryAll_Hypernyms"), readerControl=list(language="English"))
#corpus1 <- VCorpus(DirSource("20CPoetryAll_POS"), readerControl=list(language="English"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
#corpus1 <- tm_map(corpus1, content_transformer(function(x) gsub("\\$", "", x))) #run only w POS
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
corpus1 <- tm_map(corpus1, stemDocument, language = "english")
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
#remove very sparse terms
corpus1.dtm.sparse<-removeSparseTerms(corpus1.dtm, .99)
corpus1.matrix<-as.matrix(corpus1.dtm.sparse, stringsAsFactors=F)
setwd("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/01. Punctuation")
writeMM(corpus1.dtm.sparse, file="20CPoetry_Words_DTM.mtx")
library("Matrix", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
writeMM(corpus1.dtm.sparse, file="20CPoetry_Words_DTM.mtx")
save(corpus1.dtm.sparse, file="20CPoetry_Words_DTM.mtx")
a<-read.table("20CPoetry_Words_DTM.mtx")
rm(a)
rm(corpus1)
rm(results)
rm(corpus1.dtm)
rm(corpus1.matrix)
save.image("~/Documents/2. Books/Enumerations/Enumerations - Data and Code/01. Punctuation/20CPoetry.RData")
gc()
